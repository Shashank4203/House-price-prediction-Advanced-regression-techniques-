{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e528dc1a32ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['SalePrice'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgboost.XGBRegressor()\n",
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]\n",
    "\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 17.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1, nthread=None,\n",
       "                                          objective='reg:linear',\n",
       "                                          random_state=0, reg_alpha=...\n",
       "                   iid='deprecated', n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:12:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117890.47, 164790.22, 184531.08, ..., 159952.12, 121691.46,\n",
       "       235828.55], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id= pd.read_csv('test.csv')['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Id':Id,'SalePrice':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('Submission_file.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=323, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1168/1168 [==============================] - 2s 2ms/step - loss: 28638962266.3014 - val_loss: 7964165152.4384\n",
      "Epoch 2/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 9226051237.9178 - val_loss: 5349524141.3699\n",
      "Epoch 3/1000\n",
      "1168/1168 [==============================] - 1s 740us/step - loss: 6060548269.4247 - val_loss: 4678093618.4110\n",
      "Epoch 4/1000\n",
      "1168/1168 [==============================] - 1s 712us/step - loss: 4387407869.9178 - val_loss: 4185356636.4932\n",
      "Epoch 5/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 3332527732.8219 - val_loss: 4124123093.9178\n",
      "Epoch 6/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 2675679613.4795 - val_loss: 3537845130.3014\n",
      "Epoch 7/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 2220680337.3425 - val_loss: 3510066972.4932\n",
      "Epoch 8/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 1978212838.4932 - val_loss: 3230947427.2877\n",
      "Epoch 9/1000\n",
      "1168/1168 [==============================] - 1s 664us/step - loss: 1777274911.3425 - val_loss: 3140746235.1781\n",
      "Epoch 10/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 1665434871.0137 - val_loss: 3332388417.0959\n",
      "Epoch 11/1000\n",
      "1168/1168 [==============================] - 1s 890us/step - loss: 1583555386.8493 - val_loss: 3310897753.6438\n",
      "Epoch 12/1000\n",
      "1168/1168 [==============================] - 1s 699us/step - loss: 1511892686.3836 - val_loss: 3184416190.6849\n",
      "Epoch 13/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 1495420581.8630 - val_loss: 3189480988.0548\n",
      "Epoch 14/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 1470239507.7534 - val_loss: 3312663913.4247\n",
      "Epoch 15/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 1446100700.0548 - val_loss: 3342366270.6849\n",
      "Epoch 16/1000\n",
      "1168/1168 [==============================] - 1s 548us/step - loss: 1413335675.2055 - val_loss: 3239499952.2192\n",
      "Epoch 17/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 1451530823.4247 - val_loss: 3281746557.5890\n",
      "Epoch 18/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 1382385980.4521 - val_loss: 3311455135.1233\n",
      "Epoch 19/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 1381702210.6027 - val_loss: 3219863667.5068\n",
      "Epoch 20/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 1386489115.8356 - val_loss: 3178923165.9178\n",
      "Epoch 21/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 1366294938.6027 - val_loss: 3224102666.7397\n",
      "Epoch 22/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 1350678732.5205 - val_loss: 3245316231.2329\n",
      "Epoch 23/1000\n",
      "1168/1168 [==============================] - 1s 431us/step - loss: 1365773929.9041 - val_loss: 3161994806.3562\n",
      "Epoch 24/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 1339662119.9452 - val_loss: 3163912345.2055\n",
      "Epoch 25/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 1345395297.2740 - val_loss: 3106479173.4795\n",
      "Epoch 26/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 1362869804.8767 - val_loss: 3066057969.3151\n",
      "Epoch 27/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 1347901461.3699 - val_loss: 3050436082.6301\n",
      "Epoch 28/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 1315395688.8425 - val_loss: 3229295453.1507\n",
      "Epoch 29/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 1290887725.7534 - val_loss: 3408719495.0137\n",
      "Epoch 30/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 1292614650.8493 - val_loss: 3133682704.8767\n",
      "Epoch 31/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 1291034821.1233 - val_loss: 3111863669.2603\n",
      "Epoch 32/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 1315413439.4658 - val_loss: 3835257752.5479\n",
      "Epoch 33/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 1284835605.2603 - val_loss: 3035613404.2740\n",
      "Epoch 34/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 1293686881.6986 - val_loss: 3098458574.9041\n",
      "Epoch 35/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 1285153308.6849 - val_loss: 3513197288.3288\n",
      "Epoch 36/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 1268219753.0137 - val_loss: 3030192388.3836\n",
      "Epoch 37/1000\n",
      "1168/1168 [==============================] - 0s 398us/step - loss: 1237681843.3219 - val_loss: 3303506250.0822\n",
      "Epoch 38/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 1253313977.1507 - val_loss: 3073349116.0548\n",
      "Epoch 39/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 1237636506.3425 - val_loss: 3065288214.5753\n",
      "Epoch 40/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 1241097844.8082 - val_loss: 3024993316.8219\n",
      "Epoch 41/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 1202141045.2329 - val_loss: 2877820675.6164\n",
      "Epoch 42/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 1232465433.8082 - val_loss: 2899552237.5890\n",
      "Epoch 43/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 1199928699.8082 - val_loss: 2976984696.5479\n",
      "Epoch 44/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 1224659082.8356 - val_loss: 3003949545.2055\n",
      "Epoch 45/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 1209175393.3151 - val_loss: 2991162737.9726\n",
      "Epoch 46/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 1190181466.2055 - val_loss: 2976796773.2603\n",
      "Epoch 47/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 1193140557.3699 - val_loss: 2947325900.4932\n",
      "Epoch 48/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 1180841693.3836 - val_loss: 2853113216.7671\n",
      "Epoch 49/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 1170672126.6438 - val_loss: 3055598148.1644\n",
      "Epoch 50/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 1172222915.0959 - val_loss: 2780114568.9863\n",
      "Epoch 51/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 1166236951.7808 - val_loss: 2858557339.1781\n",
      "Epoch 52/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 1185278955.3973 - val_loss: 2962360504.5479\n",
      "Epoch 53/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 1133577479.3425 - val_loss: 2994692144.4384\n",
      "Epoch 54/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 1184098430.5342 - val_loss: 2967265884.0548\n",
      "Epoch 55/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 1148184468.1233 - val_loss: 2828849336.9863\n",
      "Epoch 56/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 1124465125.9247 - val_loss: 2783003316.9315\n",
      "Epoch 57/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 1138613574.5479 - val_loss: 3095921417.2055\n",
      "Epoch 58/1000\n",
      "1168/1168 [==============================] - 0s 295us/step - loss: 1137289025.9452 - val_loss: 2787993438.2466\n",
      "Epoch 59/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 1106331766.4384 - val_loss: 3131217554.8493\n",
      "Epoch 60/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 1133673677.9041 - val_loss: 2817912063.2329\n",
      "Epoch 61/1000\n",
      "1168/1168 [==============================] - 0s 274us/step - loss: 1119498425.3151 - val_loss: 2657347139.6164\n",
      "Epoch 62/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 1080055862.5890 - val_loss: 2761150274.7397\n",
      "Epoch 63/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 1055691119.2329 - val_loss: 2649425921.3151\n",
      "Epoch 64/1000\n",
      "1168/1168 [==============================] - 1s 753us/step - loss: 1098729674.0548 - val_loss: 2608913607.4521\n",
      "Epoch 65/1000\n",
      "1168/1168 [==============================] - 1s 671us/step - loss: 1080090340.1918 - val_loss: 2627809834.6301\n",
      "Epoch 66/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 1s 514us/step - loss: 1091440342.0959 - val_loss: 2638375978.5205\n",
      "Epoch 67/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 1054906339.4795 - val_loss: 2961062819.7260\n",
      "Epoch 68/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 1050188716.5890 - val_loss: 2760815126.3562\n",
      "Epoch 69/1000\n",
      "1168/1168 [==============================] - 1s 521us/step - loss: 1092609420.5205 - val_loss: 2611662668.2740\n",
      "Epoch 70/1000\n",
      "1168/1168 [==============================] - 1s 534us/step - loss: 1040853905.6712 - val_loss: 2657992579.2877\n",
      "Epoch 71/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 1064821500.1918 - val_loss: 2567661559.7808\n",
      "Epoch 72/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 1054876973.6438 - val_loss: 2522847728.9863\n",
      "Epoch 73/1000\n",
      "1168/1168 [==============================] - 1s 671us/step - loss: 1036591667.7123 - val_loss: 2507581643.2877\n",
      "Epoch 74/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 1043706475.5890 - val_loss: 2806316033.5342\n",
      "Epoch 75/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 1057186874.7260 - val_loss: 2595839664.2192\n",
      "Epoch 76/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 1044888448.0274 - val_loss: 2908291817.6438\n",
      "Epoch 77/1000\n",
      "1168/1168 [==============================] - 1s 596us/step - loss: 1063871795.7123 - val_loss: 2544395729.7534\n",
      "Epoch 78/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 1041286095.5205 - val_loss: 2519868726.3562\n",
      "Epoch 79/1000\n",
      "1168/1168 [==============================] - 1s 520us/step - loss: 972837978.4110 - val_loss: 2983524082.6301\n",
      "Epoch 80/1000\n",
      "1168/1168 [==============================] - 1s 521us/step - loss: 1065835955.1233 - val_loss: 2452436147.9452\n",
      "Epoch 81/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 1009569783.7534 - val_loss: 2508399068.7123\n",
      "Epoch 82/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 1028188727.0000 - val_loss: 2561597180.0548\n",
      "Epoch 83/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 989692915.5822 - val_loss: 2541818504.2192\n",
      "Epoch 84/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 1020677811.5068 - val_loss: 2514656819.6712\n",
      "Epoch 85/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 1002923675.5342 - val_loss: 2527146481.8630\n",
      "Epoch 86/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 984279784.1918 - val_loss: 2538320538.3562\n",
      "Epoch 87/1000\n",
      "1168/1168 [==============================] - 0s 353us/step - loss: 962526650.5890 - val_loss: 2571798427.3973\n",
      "Epoch 88/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 978871694.0685 - val_loss: 2635174229.2603\n",
      "Epoch 89/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 988054168.1507 - val_loss: 2390543407.6712\n",
      "Epoch 90/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 953431839.8904 - val_loss: 2583025960.6027\n",
      "Epoch 91/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 963378998.4932 - val_loss: 2477842085.5890\n",
      "Epoch 92/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 970137367.1370 - val_loss: 2462403339.8356\n",
      "Epoch 93/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 957789942.4658 - val_loss: 2461789364.2740\n",
      "Epoch 94/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 988280085.4247 - val_loss: 2552392445.6986\n",
      "Epoch 95/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 961170162.7123 - val_loss: 2421810695.4521\n",
      "Epoch 96/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 950081237.7945 - val_loss: 2735723472.4384\n",
      "Epoch 97/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 944256375.0137 - val_loss: 2410587825.1233\n",
      "Epoch 98/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 951644831.4384 - val_loss: 2547886639.5616\n",
      "Epoch 99/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 923040175.1233 - val_loss: 2321447105.1507\n",
      "Epoch 100/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 971831059.5616 - val_loss: 2584624200.3288\n",
      "Epoch 101/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 924919144.4521 - val_loss: 2410944611.3425\n",
      "Epoch 102/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 971417071.1918 - val_loss: 2371892924.9863\n",
      "Epoch 103/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 919768182.0411 - val_loss: 2313460465.3973\n",
      "Epoch 104/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 929330397.3562 - val_loss: 2480751588.3288\n",
      "Epoch 105/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 886263491.5342 - val_loss: 2470372620.0548\n",
      "Epoch 106/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 915801073.6301 - val_loss: 2561005621.5890\n",
      "Epoch 107/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 950874611.4247 - val_loss: 2446410627.1781\n",
      "Epoch 108/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 914828157.7603 - val_loss: 2805332555.0685\n",
      "Epoch 109/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 911123326.2740 - val_loss: 2443166239.3151\n",
      "Epoch 110/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 901084272.6712 - val_loss: 2402983097.3699\n",
      "Epoch 111/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 936916864.4247 - val_loss: 2258702937.8904\n",
      "Epoch 112/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 873738489.1096 - val_loss: 2407437580.8767\n",
      "Epoch 113/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 973400150.7260 - val_loss: 2408449856.1096\n",
      "Epoch 114/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 936228685.0959 - val_loss: 2266865349.6438\n",
      "Epoch 115/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 932511471.0479 - val_loss: 2307211572.6027\n",
      "Epoch 116/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 884015557.2055 - val_loss: 2170134807.2055\n",
      "Epoch 117/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 877555618.7123 - val_loss: 2284447690.3014\n",
      "Epoch 118/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 874328513.1233 - val_loss: 2464882341.8082\n",
      "Epoch 119/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 907680740.0274 - val_loss: 2304448676.7123\n",
      "Epoch 120/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 866579331.5068 - val_loss: 2271899432.7671\n",
      "Epoch 121/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 873463778.8630 - val_loss: 2201085925.5068\n",
      "Epoch 122/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 876128273.1781 - val_loss: 2155250279.4658\n",
      "Epoch 123/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 895999237.4658 - val_loss: 2204066901.9178\n",
      "Epoch 124/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 848791725.8904 - val_loss: 2348306125.7534\n",
      "Epoch 125/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 881793168.2192 - val_loss: 2225254928.1918\n",
      "Epoch 126/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 871052417.3288 - val_loss: 2341381064.7123\n",
      "Epoch 127/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 880401189.0959 - val_loss: 2362806290.1918\n",
      "Epoch 128/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 858734327.0822 - val_loss: 2134970876.6575\n",
      "Epoch 129/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 850801738.6712 - val_loss: 2086006733.0959\n",
      "Epoch 130/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 855486841.5479 - val_loss: 2182767246.9589\n",
      "Epoch 131/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 851801801.4932 - val_loss: 2190987324.9589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/1000\n",
      "1168/1168 [==============================] - 0s 281us/step - loss: 844605272.6370 - val_loss: 2245796057.9178\n",
      "Epoch 133/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 838193295.2671 - val_loss: 2188483704.0548\n",
      "Epoch 134/1000\n",
      "1168/1168 [==============================] - 0s 295us/step - loss: 846186116.6438 - val_loss: 2181541901.4795\n",
      "Epoch 135/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 835795141.1027 - val_loss: 2064031699.7260\n",
      "Epoch 136/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 832465781.5205 - val_loss: 2039539541.9041\n",
      "Epoch 137/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 884577604.2192 - val_loss: 2141275068.8356\n",
      "Epoch 138/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 828713262.7671 - val_loss: 1993436122.4384\n",
      "Epoch 139/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 832954971.7740 - val_loss: 2252548854.5753\n",
      "Epoch 140/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 816076418.0959 - val_loss: 2229153826.1096\n",
      "Epoch 141/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 853597576.0000 - val_loss: 2080387085.8082\n",
      "Epoch 142/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 797772830.0685 - val_loss: 2227704360.4384\n",
      "Epoch 143/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 816400468.8767 - val_loss: 2020532315.2877\n",
      "Epoch 144/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 826720142.8356 - val_loss: 2169740748.6027\n",
      "Epoch 145/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 827221889.7329 - val_loss: 2042452421.6712\n",
      "Epoch 146/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 805107938.2329 - val_loss: 1924944463.4247\n",
      "Epoch 147/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 803952361.7671 - val_loss: 2192281427.7260\n",
      "Epoch 148/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 817232175.1918 - val_loss: 1812656474.3014\n",
      "Epoch 149/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 795617882.3014 - val_loss: 1869132803.9452\n",
      "Epoch 150/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 848845555.6849 - val_loss: 1926844923.6712\n",
      "Epoch 151/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 761804251.0959 - val_loss: 2083940401.6438\n",
      "Epoch 152/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 872018094.0137 - val_loss: 1839928237.0000\n",
      "Epoch 153/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 808465911.2466 - val_loss: 1871381610.1370\n",
      "Epoch 154/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 785534147.4658 - val_loss: 1856933659.2603\n",
      "Epoch 155/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 770361268.3699 - val_loss: 1847450579.2466\n",
      "Epoch 156/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 799794713.0342 - val_loss: 1770512624.8493\n",
      "Epoch 157/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 786227385.7534 - val_loss: 1954187364.3836\n",
      "Epoch 158/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 790766649.5479 - val_loss: 1972421775.5616\n",
      "Epoch 159/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 775418240.3836 - val_loss: 1927605456.7671\n",
      "Epoch 160/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 785745071.1233 - val_loss: 1838040360.3562\n",
      "Epoch 161/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 792039315.6986 - val_loss: 1760833812.3151\n",
      "Epoch 162/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 791097339.5068 - val_loss: 1759846217.9315\n",
      "Epoch 163/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 749905732.7534 - val_loss: 1837198423.2329\n",
      "Epoch 164/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 793256872.1918 - val_loss: 1737254790.0274\n",
      "Epoch 165/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 735311080.0411 - val_loss: 1745790979.1507\n",
      "Epoch 166/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 817995606.4795 - val_loss: 1872395663.0822\n",
      "Epoch 167/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 735016455.5205 - val_loss: 1719633487.3836\n",
      "Epoch 168/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 739471090.5137 - val_loss: 1780620104.4932\n",
      "Epoch 169/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 726648237.7397 - val_loss: 1918510328.8493\n",
      "Epoch 170/1000\n",
      "1168/1168 [==============================] - 0s 274us/step - loss: 765834291.3836 - val_loss: 1741544159.3973\n",
      "Epoch 171/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 749199144.3836 - val_loss: 1854673284.0548\n",
      "Epoch 172/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 707494959.3014 - val_loss: 1756469642.1370\n",
      "Epoch 173/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 756763663.2055 - val_loss: 1767930117.8904\n",
      "Epoch 174/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 766816109.3082 - val_loss: 1740683968.1096\n",
      "Epoch 175/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 724467107.0411 - val_loss: 1718886939.9247\n",
      "Epoch 176/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 752910254.0822 - val_loss: 1706341795.2397\n",
      "Epoch 177/1000\n",
      "1168/1168 [==============================] - 0s 281us/step - loss: 727338338.7123 - val_loss: 1828433268.5890\n",
      "Epoch 178/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 742704570.5753 - val_loss: 1816161606.0274\n",
      "Epoch 179/1000\n",
      "1168/1168 [==============================] - 0s 274us/step - loss: 727380034.3151 - val_loss: 1886137520.9589\n",
      "Epoch 180/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 703928529.8082 - val_loss: 1744625613.5479\n",
      "Epoch 181/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 722428509.9452 - val_loss: 1783188830.8493\n",
      "Epoch 182/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 705506330.2877 - val_loss: 1755381297.7911\n",
      "Epoch 183/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 692402964.7466 - val_loss: 1681199008.5479\n",
      "Epoch 184/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 698982002.3493 - val_loss: 1900330649.6438\n",
      "Epoch 185/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 689361116.6233 - val_loss: 1704019583.4452\n",
      "Epoch 186/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 700063327.4521 - val_loss: 1781523569.5342\n",
      "Epoch 187/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 673057233.0753 - val_loss: 1862374399.9726\n",
      "Epoch 188/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 659821928.1438 - val_loss: 1662098021.2329\n",
      "Epoch 189/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 703862088.6301 - val_loss: 1790471606.7945\n",
      "Epoch 190/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 670135972.5753 - val_loss: 1611880136.1199\n",
      "Epoch 191/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 691882519.0000 - val_loss: 1778260714.7466\n",
      "Epoch 192/1000\n",
      "1168/1168 [==============================] - 0s 299us/step - loss: 667309293.1438 - val_loss: 1584392885.4144\n",
      "Epoch 193/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 670008101.0616 - val_loss: 1639353247.5616\n",
      "Epoch 194/1000\n",
      "1168/1168 [==============================] - 0s 281us/step - loss: 675124817.0959 - val_loss: 1713669490.3014\n",
      "Epoch 195/1000\n",
      "1168/1168 [==============================] - 0s 296us/step - loss: 665172702.0137 - val_loss: 1686343341.9726\n",
      "Epoch 196/1000\n",
      "1168/1168 [==============================] - 0s 253us/step - loss: 701832911.2192 - val_loss: 1783111679.7808\n",
      "Epoch 197/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 425us/step - loss: 685985917.6712 - val_loss: 1595029473.4247\n",
      "Epoch 198/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 628669006.7945 - val_loss: 1655726669.5342\n",
      "Epoch 199/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 645478290.1507 - val_loss: 1576503276.6027\n",
      "Epoch 200/1000\n",
      "1168/1168 [==============================] - 1s 897us/step - loss: 651033766.6301 - val_loss: 1632234220.9589\n",
      "Epoch 201/1000\n",
      "1168/1168 [==============================] - 1s 705us/step - loss: 675245244.0685 - val_loss: 1668756064.1096\n",
      "Epoch 202/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 651402076.8082 - val_loss: 1615135729.0137\n",
      "Epoch 203/1000\n",
      "1168/1168 [==============================] - 1s 644us/step - loss: 633930623.4247 - val_loss: 1868852487.2329\n",
      "Epoch 204/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 651628518.3151 - val_loss: 1767490188.0548\n",
      "Epoch 205/1000\n",
      "1168/1168 [==============================] - 1s 685us/step - loss: 641358097.7911 - val_loss: 1575399634.3527\n",
      "Epoch 206/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 667806710.4521 - val_loss: 1563801219.3151\n",
      "Epoch 207/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 626187509.3699 - val_loss: 1641678025.6438\n",
      "Epoch 208/1000\n",
      "1168/1168 [==============================] - 1s 521us/step - loss: 628065412.7397 - val_loss: 1816652311.2329\n",
      "Epoch 209/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 621993369.4760 - val_loss: 1676488766.5205\n",
      "Epoch 210/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 621690575.9863 - val_loss: 1629885804.1849\n",
      "Epoch 211/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 622373846.4726 - val_loss: 1593730778.5205\n",
      "Epoch 212/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 659065602.2740 - val_loss: 1973811955.9452\n",
      "Epoch 213/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 654863570.5068 - val_loss: 1547664889.2877\n",
      "Epoch 214/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 633080487.2055 - val_loss: 1568492791.2877\n",
      "Epoch 215/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 645117565.2877 - val_loss: 1547510713.7697\n",
      "Epoch 216/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 591910997.7260 - val_loss: 1823124359.6712\n",
      "Epoch 217/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 649593666.0685 - val_loss: 1518001864.0822\n",
      "Epoch 218/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 636288658.9178 - val_loss: 1529025952.6575\n",
      "Epoch 219/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 583841691.4795 - val_loss: 1528754081.3699\n",
      "Epoch 220/1000\n",
      "1168/1168 [==============================] - 1s 431us/step - loss: 655716979.1370 - val_loss: 1585744785.0890\n",
      "Epoch 221/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 628861868.7534 - val_loss: 1576241690.8493\n",
      "Epoch 222/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 616575827.4589 - val_loss: 1633782682.5205\n",
      "Epoch 223/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 606313456.5616 - val_loss: 1729678632.8767\n",
      "Epoch 224/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 577208122.0822 - val_loss: 1534848601.7260\n",
      "Epoch 225/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 609194456.6575 - val_loss: 1668936004.1644\n",
      "Epoch 226/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 597744510.7397 - val_loss: 1703700921.2055\n",
      "Epoch 227/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 631057760.5411 - val_loss: 1638616807.7740\n",
      "Epoch 228/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 657308232.8904 - val_loss: 1735844975.6712\n",
      "Epoch 229/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 577706821.1781 - val_loss: 1617873671.0137\n",
      "Epoch 230/1000\n",
      "1168/1168 [==============================] - 1s 452us/step - loss: 599724618.2808 - val_loss: 1613809495.5616\n",
      "Epoch 231/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 606910993.5034 - val_loss: 1581161950.7158\n",
      "Epoch 232/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 575000351.0959 - val_loss: 1461059823.1781\n",
      "Epoch 233/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 592428318.0822 - val_loss: 1601370578.4658\n",
      "Epoch 234/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 605395470.1438 - val_loss: 1558093045.5890\n",
      "Epoch 235/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 585834170.4384 - val_loss: 1565937303.6712\n",
      "Epoch 236/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 589055099.2466 - val_loss: 1473308813.6986\n",
      "Epoch 237/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 606681755.5342 - val_loss: 1647024365.3699\n",
      "Epoch 238/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 560086878.6575 - val_loss: 1476042912.9863\n",
      "Epoch 239/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 563978292.8014 - val_loss: 1443236256.0548\n",
      "Epoch 240/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 584699266.5342 - val_loss: 1604643460.6027\n",
      "Epoch 241/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 591922883.1507 - val_loss: 1511742245.0411\n",
      "Epoch 242/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 565139400.0822 - val_loss: 1526568355.9452\n",
      "Epoch 243/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 557758224.5274 - val_loss: 1698809550.8493\n",
      "Epoch 244/1000\n",
      "1168/1168 [==============================] - 1s 438us/step - loss: 567450900.0137 - val_loss: 1466839949.9726\n",
      "Epoch 245/1000\n",
      "1168/1168 [==============================] - 1s 452us/step - loss: 524001713.0068 - val_loss: 1430971283.7808\n",
      "Epoch 246/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 563292365.0000 - val_loss: 1448960410.3014\n",
      "Epoch 247/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 544970639.9041 - val_loss: 1446158054.5753\n",
      "Epoch 248/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 535299493.2055 - val_loss: 1450942058.3014\n",
      "Epoch 249/1000\n",
      "1168/1168 [==============================] - 0s 418us/step - loss: 558102921.6849 - val_loss: 1435845750.2192\n",
      "Epoch 250/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 546694514.6575 - val_loss: 1495427322.6207\n",
      "Epoch 251/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 574916980.0000 - val_loss: 1459101289.9726\n",
      "Epoch 252/1000\n",
      "1168/1168 [==============================] - 1s 431us/step - loss: 531953191.7603 - val_loss: 1533950611.6233\n",
      "Epoch 253/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 550632373.8836 - val_loss: 1394228323.5616\n",
      "Epoch 254/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 555959251.9863 - val_loss: 1477524190.5753\n",
      "Epoch 255/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 539859196.1781 - val_loss: 1511539608.3288\n",
      "Epoch 256/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 553590678.1781 - val_loss: 1485456141.7534\n",
      "Epoch 257/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 528452269.5753 - val_loss: 1487978832.0000\n",
      "Epoch 258/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 603412210.2055 - val_loss: 1546018346.1918\n",
      "Epoch 259/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 548294637.6644 - val_loss: 1377798475.6164\n",
      "Epoch 260/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 508319630.7842 - val_loss: 1437331739.1781\n",
      "Epoch 261/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 525311628.6849 - val_loss: 1342376653.3973\n",
      "Epoch 262/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 397us/step - loss: 498125970.9452 - val_loss: 1331049904.4384\n",
      "Epoch 263/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 537541857.0753 - val_loss: 1427919522.7671\n",
      "Epoch 264/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 542352360.9726 - val_loss: 1427977047.0342\n",
      "Epoch 265/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 543525636.2603 - val_loss: 1402964553.4795\n",
      "Epoch 266/1000\n",
      "1168/1168 [==============================] - 1s 500us/step - loss: 507984812.0959 - val_loss: 1429193880.5479\n",
      "Epoch 267/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 491359440.9178 - val_loss: 1394331366.6096\n",
      "Epoch 268/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 503906910.1233 - val_loss: 1439689326.7483\n",
      "Epoch 269/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 500866743.8630 - val_loss: 1673254350.4658\n",
      "Epoch 270/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 558048150.7123 - val_loss: 1391001020.4867\n",
      "Epoch 271/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 469730652.5890 - val_loss: 1441350704.7123\n",
      "Epoch 272/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 477212547.7123 - val_loss: 1366783966.3861\n",
      "Epoch 273/1000\n",
      "1168/1168 [==============================] - 1s 644us/step - loss: 516921947.0753 - val_loss: 1376850652.6113\n",
      "Epoch 274/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 484839193.4110 - val_loss: 1599012153.4247\n",
      "Epoch 275/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 518530031.3082 - val_loss: 1303384228.6849\n",
      "Epoch 276/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 484706437.3836 - val_loss: 1419431050.3014\n",
      "Epoch 277/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 485848297.5548 - val_loss: 1452639215.6712\n",
      "Epoch 278/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 507025430.0959 - val_loss: 1288431969.8082\n",
      "Epoch 279/1000\n",
      "1168/1168 [==============================] - 1s 664us/step - loss: 500538350.9452 - val_loss: 1365086825.6164\n",
      "Epoch 280/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 505589476.3288 - val_loss: 1369325079.6712\n",
      "Epoch 281/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 496558551.9452 - val_loss: 1333750074.1918\n",
      "Epoch 282/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 463363218.2740 - val_loss: 1300700795.1781\n",
      "Epoch 283/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 465117940.3014 - val_loss: 1355094127.7363\n",
      "Epoch 284/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 476370786.8356 - val_loss: 1318373878.9589\n",
      "Epoch 285/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 466940581.9247 - val_loss: 1358811309.2808\n",
      "Epoch 286/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 466657057.2603 - val_loss: 1264775966.6849\n",
      "Epoch 287/1000\n",
      "1168/1168 [==============================] - 1s 719us/step - loss: 485317894.9863 - val_loss: 1329586785.6986\n",
      "Epoch 288/1000\n",
      "1168/1168 [==============================] - 1s 753us/step - loss: 492377146.3562 - val_loss: 1272117586.5342\n",
      "Epoch 289/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 513923181.7123 - val_loss: 1241810766.3562\n",
      "Epoch 290/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 482250112.7671 - val_loss: 1394591369.5342\n",
      "Epoch 291/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 466607467.4247 - val_loss: 1342806317.3459\n",
      "Epoch 292/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 483375830.6781 - val_loss: 1247612095.0411\n",
      "Epoch 293/1000\n",
      "1168/1168 [==============================] - 1s 596us/step - loss: 463888234.1027 - val_loss: 1231664783.0137\n",
      "Epoch 294/1000\n",
      "1168/1168 [==============================] - 1s 438us/step - loss: 469568873.3425 - val_loss: 1256633797.4795\n",
      "Epoch 295/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 464331356.5479 - val_loss: 1214807757.4247\n",
      "Epoch 296/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 432217784.5137 - val_loss: 1364786757.1267\n",
      "Epoch 297/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 490424522.3699 - val_loss: 1238992339.5068\n",
      "Epoch 298/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 455293881.8425 - val_loss: 1237354620.1096\n",
      "Epoch 299/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 449958148.5068 - val_loss: 1357561697.9726\n",
      "Epoch 300/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 447980108.897 - 1s 459us/step - loss: 446405445.4315 - val_loss: 1411310099.0685\n",
      "Epoch 301/1000\n",
      "1168/1168 [==============================] - 1s 452us/step - loss: 455215494.2055 - val_loss: 1256843542.1781\n",
      "Epoch 302/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 448377626.1986 - val_loss: 1347138878.4863\n",
      "Epoch 303/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 473581158.6438 - val_loss: 1224612767.0479\n",
      "Epoch 304/1000\n",
      "1168/1168 [==============================] - 1s 458us/step - loss: 458004008.4110 - val_loss: 1230249628.7671\n",
      "Epoch 305/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 457375460.7123 - val_loss: 1235453549.9178\n",
      "Epoch 306/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 431951835.2260 - val_loss: 1285475454.6575\n",
      "Epoch 307/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 481072540.1507 - val_loss: 1329622570.1918\n",
      "Epoch 308/1000\n",
      "1168/1168 [==============================] - 1s 500us/step - loss: 424463118.5000 - val_loss: 1205740580.6575\n",
      "Epoch 309/1000\n",
      "1168/1168 [==============================] - 1s 438us/step - loss: 425619887.4212 - val_loss: 1315748715.0822\n",
      "Epoch 310/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 477928808.8493 - val_loss: 1274953638.4658\n",
      "Epoch 311/1000\n",
      "1168/1168 [==============================] - 1s 541us/step - loss: 523681939.3288 - val_loss: 1169195605.4247\n",
      "Epoch 312/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 466484328.5616 - val_loss: 1199644191.6712\n",
      "Epoch 313/1000\n",
      "1168/1168 [==============================] - 0s 418us/step - loss: 444838186.6301 - val_loss: 1251342257.8767\n",
      "Epoch 314/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 422817534.5753 - val_loss: 1230612005.0959\n",
      "Epoch 315/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 438446774.7945 - val_loss: 1270055794.5205\n",
      "Epoch 316/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 437696224.5000 - val_loss: 1305066790.9041\n",
      "Epoch 317/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 427472672.1712 - val_loss: 1443106595.3699\n",
      "Epoch 318/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 437146168.5753 - val_loss: 1233519020.3836\n",
      "Epoch 319/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 418046703.5890 - val_loss: 1209866071.1781\n",
      "Epoch 320/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 423877382.8082 - val_loss: 1302356796.0000\n",
      "Epoch 321/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 417623102.1199 - val_loss: 1229882218.8493\n",
      "Epoch 322/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 414305226.5822 - val_loss: 1151761330.5753\n",
      "Epoch 323/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 426078488.4178 - val_loss: 1276486344.3836\n",
      "Epoch 324/1000\n",
      "1168/1168 [==============================] - 0s 281us/step - loss: 429768404.1918 - val_loss: 1161494492.4932\n",
      "Epoch 325/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 398047260.0890 - val_loss: 1306729663.9726\n",
      "Epoch 326/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 406318535.0342 - val_loss: 1236695855.8356\n",
      "Epoch 327/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 301us/step - loss: 399049534.6644 - val_loss: 1339402263.7808\n",
      "Epoch 328/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 413240737.7192 - val_loss: 1153269425.2688\n",
      "Epoch 329/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 406351347.4384 - val_loss: 1460842919.0137\n",
      "Epoch 330/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 417998269.0274 - val_loss: 1228742408.6712\n",
      "Epoch 331/1000\n",
      "1168/1168 [==============================] - 0s 253us/step - loss: 414339800.7945 - val_loss: 1212623869.6438\n",
      "Epoch 332/1000\n",
      "1168/1168 [==============================] - 0s 253us/step - loss: 427165418.6301 - val_loss: 1164006789.3973\n",
      "Epoch 333/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 416120070.5342 - val_loss: 1175276279.2329\n",
      "Epoch 334/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 414587656.5548 - val_loss: 1169620610.5205\n",
      "Epoch 335/1000\n",
      "1168/1168 [==============================] - 1s 815us/step - loss: 421543927.3288 - val_loss: 1160527424.1096\n",
      "Epoch 336/1000\n",
      "1168/1168 [==============================] - 1s 726us/step - loss: 413742936.9863 - val_loss: 1123885221.6986\n",
      "Epoch 337/1000\n",
      "1168/1168 [==============================] - 1s 705us/step - loss: 390755690.2603 - val_loss: 1206169885.0959\n",
      "Epoch 338/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 372711771.6610 - val_loss: 1141261207.7808\n",
      "Epoch 339/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 405203241.5822 - val_loss: 1199136646.1370\n",
      "Epoch 340/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 391070014.2603 - val_loss: 1212299841.1507\n",
      "Epoch 341/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 376086489.8425 - val_loss: 1212084014.9589\n",
      "Epoch 342/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 537825231.2603 - val_loss: 1320470171.3425\n",
      "Epoch 343/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 409334676.7466 - val_loss: 1274083577.5342\n",
      "Epoch 344/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 394866632.9178 - val_loss: 1168309038.5479\n",
      "Epoch 345/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 396853461.6781 - val_loss: 1099240263.7808\n",
      "Epoch 346/1000\n",
      "1168/1168 [==============================] - 1s 685us/step - loss: 386260834.2945 - val_loss: 1332851543.0137\n",
      "Epoch 347/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 369295363.4521 - val_loss: 1169937396.6575\n",
      "Epoch 348/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 368148828.8356 - val_loss: 1189092579.8904\n",
      "Epoch 349/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 366016040.3699 - val_loss: 1170146428.7123\n",
      "Epoch 350/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 381735179.7466 - val_loss: 1125509794.8356\n",
      "Epoch 351/1000\n",
      "1168/1168 [==============================] - 0s 418us/step - loss: 406986106.5753 - val_loss: 1104774878.0274\n",
      "Epoch 352/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 371635221.6884 - val_loss: 1254519534.1918\n",
      "Epoch 353/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 391208934.5959 - val_loss: 1097439674.3288\n",
      "Epoch 354/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 383282786.8356 - val_loss: 1208511234.5205\n",
      "Epoch 355/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 375890118.4418 - val_loss: 1147579816.3836\n",
      "Epoch 356/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 371624712.2671 - val_loss: 1231005313.6644\n",
      "Epoch 357/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 357089823.0411 - val_loss: 1136432438.7397\n",
      "Epoch 358/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 393570146.4521 - val_loss: 1188214952.1096\n",
      "Epoch 359/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 361903840.2808 - val_loss: 1154112454.0274\n",
      "Epoch 360/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 419809382.5411 - val_loss: 1163420571.1781\n",
      "Epoch 361/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 364556082.5411 - val_loss: 1116801005.5342\n",
      "Epoch 362/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 382041274.0685 - val_loss: 1233655308.9315\n",
      "Epoch 363/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 357822766.1712 - val_loss: 1220899746.5205\n",
      "Epoch 364/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 345494539.5342 - val_loss: 1178748479.9452\n",
      "Epoch 365/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 384859326.0274 - val_loss: 1314303776.7466\n",
      "Epoch 366/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 381804617.0000 - val_loss: 1097313653.5890\n",
      "Epoch 367/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 381125003.7192 - val_loss: 1165859429.3699\n",
      "Epoch 368/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 372460876.1986 - val_loss: 1098656466.1370\n",
      "Epoch 369/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 345100607.3699 - val_loss: 1212950652.1370\n",
      "Epoch 370/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 354877215.4932 - val_loss: 1127180947.8356\n",
      "Epoch 371/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 344760388.0274 - val_loss: 1126946225.9726\n",
      "Epoch 372/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 349379573.3253 - val_loss: 1277298439.6164\n",
      "Epoch 373/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 399464402.8699 - val_loss: 1283240229.2603\n",
      "Epoch 374/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 368911800.6849 - val_loss: 1168149630.3562\n",
      "Epoch 375/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 351609780.1027 - val_loss: 1234926626.4110\n",
      "Epoch 376/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 341904631.7055 - val_loss: 1187751427.7808\n",
      "Epoch 377/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 334699137.8493 - val_loss: 1202566957.9178\n",
      "Epoch 378/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 342692771.3425 - val_loss: 1146314133.6986\n",
      "Epoch 379/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 345684920.7055 - val_loss: 1128950945.1507\n",
      "Epoch 380/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 342367191.9178 - val_loss: 1105976883.5616\n",
      "Epoch 381/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 361972080.4315 - val_loss: 1141755133.4795\n",
      "Epoch 382/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 355817508.0068 - val_loss: 1219168797.1644\n",
      "Epoch 383/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 350747332.2877 - val_loss: 1102398264.5479\n",
      "Epoch 384/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 333972471.8904 - val_loss: 1168132627.4521\n",
      "Epoch 385/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 341318279.4110 - val_loss: 1205935149.5890\n",
      "Epoch 386/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 327885558.5411 - val_loss: 1136958185.5342\n",
      "Epoch 387/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 376107047.3014 - val_loss: 1086945048.1096\n",
      "Epoch 388/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 370861615.0274 - val_loss: 1212326892.3562\n",
      "Epoch 389/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 378204177.0205 - val_loss: 1126408406.5753\n",
      "Epoch 390/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 326325480.4110 - val_loss: 1107057888.4384\n",
      "Epoch 391/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 337893106.2329 - val_loss: 1188249211.9452\n",
      "Epoch 392/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 294us/step - loss: 321599587.0753 - val_loss: 1074305743.5616\n",
      "Epoch 393/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 331877410.7808 - val_loss: 1107278836.8219\n",
      "Epoch 394/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 364343738.3973 - val_loss: 1132990893.1781\n",
      "Epoch 395/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 360835296.6027 - val_loss: 1043662177.0411\n",
      "Epoch 396/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 350137915.3288 - val_loss: 1127415126.0822\n",
      "Epoch 397/1000\n",
      "1168/1168 [==============================] - 1s 664us/step - loss: 317098367.6918 - val_loss: 1066480612.0548\n",
      "Epoch 398/1000\n",
      "1168/1168 [==============================] - 1s 685us/step - loss: 318571852.1438 - val_loss: 1063533034.3014\n",
      "Epoch 399/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 330127111.6507 - val_loss: 1060732735.2329\n",
      "Epoch 400/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 349207375.7603 - val_loss: 1111557168.4658\n",
      "Epoch 401/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 327874642.0685 - val_loss: 1081069711.1233\n",
      "Epoch 402/1000\n",
      "1168/1168 [==============================] - 1s 534us/step - loss: 341422063.5377 - val_loss: 1147864806.6849\n",
      "Epoch 403/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 340752879.7397 - val_loss: 1124680919.6712\n",
      "Epoch 404/1000\n",
      "1168/1168 [==============================] - 1s 699us/step - loss: 346295841.8493 - val_loss: 1395413461.8288\n",
      "Epoch 405/1000\n",
      "1168/1168 [==============================] - 1s 523us/step - loss: 341770176.9521 - val_loss: 1096501204.8219\n",
      "Epoch 406/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 313610358.678 - 1s 514us/step - loss: 313919438.9863 - val_loss: 1068519136.7671\n",
      "Epoch 407/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 337284625.1233 - val_loss: 1086115596.8767\n",
      "Epoch 408/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 327295666.8493 - val_loss: 1151343957.4795\n",
      "Epoch 409/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 344707144.7568 - val_loss: 1078818936.7671\n",
      "Epoch 410/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 319830508.3630 - val_loss: 1155608191.4178\n",
      "Epoch 411/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 423292972.7945 - val_loss: 1179875772.6027\n",
      "Epoch 412/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 327782075.2055 - val_loss: 1082709384.3836\n",
      "Epoch 413/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 312582348.2192 - val_loss: 1135056365.0685\n",
      "Epoch 414/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 324502347.8836 - val_loss: 1128767143.5753\n",
      "Epoch 415/1000\n",
      "1168/1168 [==============================] - 1s 438us/step - loss: 315453923.9384 - val_loss: 1124955739.7808\n",
      "Epoch 416/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 329769295.0205 - val_loss: 1095464316.8767\n",
      "Epoch 417/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 324946576.2260 - val_loss: 1098981241.6575\n",
      "Epoch 418/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 303004716.2945 - val_loss: 1081143628.9589\n",
      "Epoch 419/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 329179610.6712 - val_loss: 1158579697.8836\n",
      "Epoch 420/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 311295638.3151 - val_loss: 1105940901.5616\n",
      "Epoch 421/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 338393449.5993 - val_loss: 1134894032.7945\n",
      "Epoch 422/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 323833880.9521 - val_loss: 1141282719.0685\n",
      "Epoch 423/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 315007325.6301 - val_loss: 1087917740.8767\n",
      "Epoch 424/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 319999383.1781 - val_loss: 1167369814.3562\n",
      "Epoch 425/1000\n",
      "1168/1168 [==============================] - 1s 596us/step - loss: 324215624.0685 - val_loss: 1197765613.5068\n",
      "Epoch 426/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 305194668.7842 - val_loss: 937603478.3562\n",
      "Epoch 427/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 323362499.3699 - val_loss: 1055957124.9863\n",
      "Epoch 428/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 324398245.3767 - val_loss: 1188258021.0548\n",
      "Epoch 429/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 315958255.6096 - val_loss: 1014483961.0959\n",
      "Epoch 430/1000\n",
      "1168/1168 [==============================] - 1s 699us/step - loss: 323105626.1918 - val_loss: 1122009196.0548\n",
      "Epoch 431/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 295300433.0274 - val_loss: 1139397117.2466\n",
      "Epoch 432/1000\n",
      "1168/1168 [==============================] - 1s 733us/step - loss: 312395959.2397 - val_loss: 1176977411.0137\n",
      "Epoch 433/1000\n",
      "1168/1168 [==============================] - 1s 657us/step - loss: 319838552.3699 - val_loss: 1058318108.4932\n",
      "Epoch 434/1000\n",
      "1168/1168 [==============================] - 1s 671us/step - loss: 309512827.9692 - val_loss: 1051665293.5890\n",
      "Epoch 435/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 341135913.8767 - val_loss: 1007191530.0822\n",
      "Epoch 436/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 315283448.8425 - val_loss: 987049944.3836\n",
      "Epoch 437/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 304214922.3390 - val_loss: 1100349954.4110\n",
      "Epoch 438/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 307529117.2397 - val_loss: 1048029198.8219\n",
      "Epoch 439/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 313314941.7568 - val_loss: 1104030103.0685\n",
      "Epoch 440/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 335968977.2877 - val_loss: 1080996168.6575\n",
      "Epoch 441/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 302777177.5925 - val_loss: 999667813.2329\n",
      "Epoch 442/1000\n",
      "1168/1168 [==============================] - 0s 253us/step - loss: 290007771.2055 - val_loss: 1029805713.6986\n",
      "Epoch 443/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 309475040.2055 - val_loss: 1079940735.5616\n",
      "Epoch 444/1000\n",
      "1168/1168 [==============================] - 0s 240us/step - loss: 328673095.2945 - val_loss: 1006973826.3562\n",
      "Epoch 445/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 291284062.0685 - val_loss: 952923089.2055\n",
      "Epoch 446/1000\n",
      "1168/1168 [==============================] - 0s 240us/step - loss: 293891607.5548 - val_loss: 1036623364.4932\n",
      "Epoch 447/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 292025810.845 - 0s 199us/step - loss: 310220686.5959 - val_loss: 1122712961.3151\n",
      "Epoch 448/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 298632421.4384 - val_loss: 1093487985.2055\n",
      "Epoch 449/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 291779134.8767 - val_loss: 1100723815.9726\n",
      "Epoch 450/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 302971246.1336 - val_loss: 1174485677.1627\n",
      "Epoch 451/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 291328028.1438 - val_loss: 1020556892.3288\n",
      "Epoch 452/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 280502723.4315 - val_loss: 1132181389.4007\n",
      "Epoch 453/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 293339350.7534 - val_loss: 1111850104.8082\n",
      "Epoch 454/1000\n",
      "1168/1168 [==============================] - 0s 233us/step - loss: 293184396.2568 - val_loss: 987740721.3151\n",
      "Epoch 455/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 280951565.1678 - val_loss: 1062744356.6027\n",
      "Epoch 456/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 319068511.6712 - val_loss: 1036644868.9315\n",
      "Epoch 457/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 226us/step - loss: 290257642.0959 - val_loss: 979858695.1781\n",
      "Epoch 458/1000\n",
      "1168/1168 [==============================] - 0s 240us/step - loss: 294584700.7158 - val_loss: 966609350.4110\n",
      "Epoch 459/1000\n",
      "1168/1168 [==============================] - 1s 699us/step - loss: 281485760.7466 - val_loss: 1014154603.7808\n",
      "Epoch 460/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 296001210.7705 - val_loss: 990903830.9315\n",
      "Epoch 461/1000\n",
      "1168/1168 [==============================] - 1s 685us/step - loss: 284758533.4041 - val_loss: 1127411858.0000\n",
      "Epoch 462/1000\n",
      "1168/1168 [==============================] - 1s 685us/step - loss: 299015012.7329 - val_loss: 1027596073.3699\n",
      "Epoch 463/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 335002203.439 - 1s 685us/step - loss: 334334805.7397 - val_loss: 1003175061.4795\n",
      "Epoch 464/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 315444437.5890 - val_loss: 989346439.2329\n",
      "Epoch 465/1000\n",
      "1168/1168 [==============================] - 1s 644us/step - loss: 294258006.3973 - val_loss: 1015951949.0411\n",
      "Epoch 466/1000\n",
      "1168/1168 [==============================] - 1s 733us/step - loss: 286887867.3219 - val_loss: 1095635307.0685\n",
      "Epoch 467/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 286997331.8767 - val_loss: 1021437210.1370\n",
      "Epoch 468/1000\n",
      "1168/1168 [==============================] - 1s 562us/step - loss: 340192078.3356 - val_loss: 1226286457.3014\n",
      "Epoch 469/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 309667751.7466 - val_loss: 951323019.3425\n",
      "Epoch 470/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 298888002.6781 - val_loss: 1156604319.1233\n",
      "Epoch 471/1000\n",
      "1168/1168 [==============================] - 1s 521us/step - loss: 308162424.9229 - val_loss: 894082099.2329\n",
      "Epoch 472/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 306140033.7260 - val_loss: 1075797049.5342\n",
      "Epoch 473/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 284965930.5137 - val_loss: 1055670977.5342\n",
      "Epoch 474/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 290455027.2055 - val_loss: 959442417.9178\n",
      "Epoch 475/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 301548834.8733 - val_loss: 1068732497.4521\n",
      "Epoch 476/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 282748723.8664 - val_loss: 1087953585.3014\n",
      "Epoch 477/1000\n",
      "1168/1168 [==============================] - 1s 431us/step - loss: 281007528.7911 - val_loss: 994001934.5205\n",
      "Epoch 478/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 292219004.4932 - val_loss: 1121046254.7568\n",
      "Epoch 479/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 274293548.1918 - val_loss: 984863446.4110\n",
      "Epoch 480/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 307690712.0411 - val_loss: 1004521951.0137\n",
      "Epoch 481/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 285950764.9521 - val_loss: 1001898984.4384\n",
      "Epoch 482/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 277735439.7808 - val_loss: 1066889816.6575\n",
      "Epoch 483/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 275496988.5976 - val_loss: 988306695.1781\n",
      "Epoch 484/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 282547335.4726 - val_loss: 1054744989.2055\n",
      "Epoch 485/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 283729931.3630 - val_loss: 1084990305.4795\n",
      "Epoch 486/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 274137065.7603 - val_loss: 1037094302.0274\n",
      "Epoch 487/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 284133201.9658 - val_loss: 969046604.9315\n",
      "Epoch 488/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 264148702.5925 - val_loss: 1231956914.5890\n",
      "Epoch 489/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 288963627.6781 - val_loss: 1013395370.2466\n",
      "Epoch 490/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 293747328.2226 - val_loss: 1013509764.4932\n",
      "Epoch 491/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 283246030.1199 - val_loss: 1096666400.4384\n",
      "Epoch 492/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 288473958.1062 - val_loss: 975476166.0000\n",
      "Epoch 493/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 263394560.1507 - val_loss: 1076817434.7397\n",
      "Epoch 494/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 279078567.4110 - val_loss: 889505601.5342\n",
      "Epoch 495/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 306149033.6712 - val_loss: 1041201558.4932\n",
      "Epoch 496/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 304751523.4178 - val_loss: 1173263232.7701\n",
      "Epoch 497/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 269752873.2568 - val_loss: 992464475.2329\n",
      "Epoch 498/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 280743160.2671 - val_loss: 1189333886.4229\n",
      "Epoch 499/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 296490738.0411 - val_loss: 981691485.7260\n",
      "Epoch 500/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 285990853.1712 - val_loss: 1012839416.1644\n",
      "Epoch 501/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 267596315.4863 - val_loss: 1130913095.4212\n",
      "Epoch 502/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 279472972.7740 - val_loss: 1092172569.6729\n",
      "Epoch 503/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 297275654.3014 - val_loss: 1053077762.1644\n",
      "Epoch 504/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 258743853.8493 - val_loss: 1002359085.4041\n",
      "Epoch 505/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 300487712.8870 - val_loss: 1000333395.2877\n",
      "Epoch 506/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 311049734.0822 - val_loss: 951318560.5479\n",
      "Epoch 507/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 261501057.0548 - val_loss: 990602236.1918\n",
      "Epoch 508/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 261517336.3767 - val_loss: 935495638.7671\n",
      "Epoch 509/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 263475887.7534 - val_loss: 1196713458.1019\n",
      "Epoch 510/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 261693816.1575 - val_loss: 1050954564.6027\n",
      "Epoch 511/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 273895098.0137 - val_loss: 1181476956.6301\n",
      "Epoch 512/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 284410742.2671 - val_loss: 992780280.7123\n",
      "Epoch 513/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 302271303.7226 - val_loss: 1087387158.7945\n",
      "Epoch 514/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 262265778.6370 - val_loss: 1079375642.3699\n",
      "Epoch 515/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 278899512.2945 - val_loss: 1059648258.6849\n",
      "Epoch 516/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 274914567.6644 - val_loss: 938938399.3973\n",
      "Epoch 517/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 270882620.2500 - val_loss: 1055351332.2466\n",
      "Epoch 518/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 269866009.8630 - val_loss: 1239151540.7945\n",
      "Epoch 519/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 300119180.8151 - val_loss: 1136187327.6164\n",
      "Epoch 520/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 264964433.3699 - val_loss: 1079922024.3425\n",
      "Epoch 521/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 251202069.2329 - val_loss: 997472734.2466\n",
      "Epoch 522/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 274us/step - loss: 270253180.2466 - val_loss: 1086922195.4384\n",
      "Epoch 523/1000\n",
      "1168/1168 [==============================] - 0s 233us/step - loss: 270593933.6781 - val_loss: 973108541.9178\n",
      "Epoch 524/1000\n",
      "1168/1168 [==============================] - 0s 233us/step - loss: 265461407.6986 - val_loss: 1023525380.8767\n",
      "Epoch 525/1000\n",
      "1168/1168 [==============================] - 1s 596us/step - loss: 268340496.0685 - val_loss: 984691193.8630\n",
      "Epoch 526/1000\n",
      "1168/1168 [==============================] - 1s 726us/step - loss: 251130375.0154 - val_loss: 988636681.0411\n",
      "Epoch 527/1000\n",
      "1168/1168 [==============================] - 1s 699us/step - loss: 265711959.4932 - val_loss: 993220703.5616\n",
      "Epoch 528/1000\n",
      "1168/1168 [==============================] - 1s 712us/step - loss: 264315731.0377 - val_loss: 991914153.8630\n",
      "Epoch 529/1000\n",
      "1168/1168 [==============================] - 1s 699us/step - loss: 267561878.1918 - val_loss: 1014670271.1507\n",
      "Epoch 530/1000\n",
      "1168/1168 [==============================] - 1s 726us/step - loss: 264493294.3219 - val_loss: 1002342707.8356\n",
      "Epoch 531/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 256313523.3562 - val_loss: 1005718309.1507\n",
      "Epoch 532/1000\n",
      "1168/1168 [==============================] - 1s 562us/step - loss: 253678055.8288 - val_loss: 1085117576.7671\n",
      "Epoch 533/1000\n",
      "1168/1168 [==============================] - 1s 626us/step - loss: 261828170.8288 - val_loss: 1068407692.8219\n",
      "Epoch 534/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 278968161.4418 - val_loss: 1043883131.7945\n",
      "Epoch 535/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 325199640.5411 - val_loss: 1010470387.2329\n",
      "Epoch 536/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 294099392.7945 - val_loss: 1015121567.0137\n",
      "Epoch 537/1000\n",
      "1168/1168 [==============================] - 1s 541us/step - loss: 252338618.8014 - val_loss: 1041047502.4110\n",
      "Epoch 538/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 260123479.6575 - val_loss: 1140825062.1849\n",
      "Epoch 539/1000\n",
      "1168/1168 [==============================] - 1s 534us/step - loss: 271310696.8425 - val_loss: 1206759340.8904\n",
      "Epoch 540/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 252495829.1918 - val_loss: 1041491565.4247\n",
      "Epoch 541/1000\n",
      "1168/1168 [==============================] - 0s 418us/step - loss: 255063600.1233 - val_loss: 1055322344.2740\n",
      "Epoch 542/1000\n",
      "1168/1168 [==============================] - 0s 426us/step - loss: 249771416.1370 - val_loss: 1013882244.0000\n",
      "Epoch 543/1000\n",
      "1168/1168 [==============================] - 0s 396us/step - loss: 269983455.2192 - val_loss: 1101686367.1233\n",
      "Epoch 544/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 253508373.7055 - val_loss: 1062189072.4384\n",
      "Epoch 545/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 269297154.1438 - val_loss: 1089397459.6712\n",
      "Epoch 546/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 254250361.5308 - val_loss: 1012604661.6712\n",
      "Epoch 547/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 261771178.1644 - val_loss: 1145857138.8836\n",
      "Epoch 548/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 261719131.5274 - val_loss: 986728917.6986\n",
      "Epoch 549/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 260914826.8356 - val_loss: 956069664.9315\n",
      "Epoch 550/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 317231795.6233 - val_loss: 928309468.2192\n",
      "Epoch 551/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 263331150.1678 - val_loss: 1064165894.6849\n",
      "Epoch 552/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 268037931.4178 - val_loss: 1073722580.9041\n",
      "Epoch 553/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 251779947.2671 - val_loss: 1015947082.5753\n",
      "Epoch 554/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 286235832.2123 - val_loss: 1026712003.2466\n",
      "Epoch 555/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 265553100.2945 - val_loss: 1074334349.4658\n",
      "Epoch 556/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 242926955.4212 - val_loss: 997672861.7534\n",
      "Epoch 557/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 242759844.0719 - val_loss: 1088803564.9726\n",
      "Epoch 558/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 259513449.7534 - val_loss: 1003121646.5753\n",
      "Epoch 559/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 344873810.0342 - val_loss: 927745146.2192\n",
      "Epoch 560/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 258771938.4589 - val_loss: 1052918882.4932\n",
      "Epoch 561/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 239047379.5685 - val_loss: 971375493.5616\n",
      "Epoch 562/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 259516175.3904 - val_loss: 1214253774.6849\n",
      "Epoch 563/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 246913271.0240 - val_loss: 1119303408.5548\n",
      "Epoch 564/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 271114271.3904 - val_loss: 1017620896.1096\n",
      "Epoch 565/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 244258880.4538 - val_loss: 1042544428.9863\n",
      "Epoch 566/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 257725687.6849 - val_loss: 1045314413.6986\n",
      "Epoch 567/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 255181381.1233 - val_loss: 996364795.6164\n",
      "Epoch 568/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 248491464.9452 - val_loss: 1068765877.9315\n",
      "Epoch 569/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 267638566.0651 - val_loss: 1079003238.1096\n",
      "Epoch 570/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 271257505.1267 - val_loss: 1031328191.7123\n",
      "Epoch 571/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 257427646.7260 - val_loss: 931604081.3699\n",
      "Epoch 572/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 272413725.9932 - val_loss: 1025291489.5205\n",
      "Epoch 573/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 244049107.6301 - val_loss: 1024749716.7671\n",
      "Epoch 574/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 253865509.0685 - val_loss: 937844232.0000\n",
      "Epoch 575/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 245970545.9178 - val_loss: 911957473.2055\n",
      "Epoch 576/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 244222469.5034 - val_loss: 962122680.4521\n",
      "Epoch 577/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 227389404.3459 - val_loss: 974373303.8904\n",
      "Epoch 578/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 250947795.3116 - val_loss: 1100690806.2192\n",
      "Epoch 579/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 244686962.1062 - val_loss: 998950641.2055\n",
      "Epoch 580/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 233413660.5342 - val_loss: 973039510.1918\n",
      "Epoch 581/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 241276897.9229 - val_loss: 1008199933.1096\n",
      "Epoch 582/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 231109377.5103 - val_loss: 976374896.9315\n",
      "Epoch 583/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 237721404.1610 - val_loss: 918750281.7534\n",
      "Epoch 584/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 237659551.0205 - val_loss: 1089536304.6712\n",
      "Epoch 585/1000\n",
      "1168/1168 [==============================] - 0s 274us/step - loss: 262228943.6644 - val_loss: 949068107.9452\n",
      "Epoch 586/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 246696731.7877 - val_loss: 958306212.1096\n",
      "Epoch 587/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 239373220.4110 - val_loss: 1041725918.1233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 588/1000\n",
      "1168/1168 [==============================] - 0s 199us/step - loss: 238570686.6918 - val_loss: 1079981712.7397\n",
      "Epoch 589/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 288942904.1849 - val_loss: 1008099002.6849\n",
      "Epoch 590/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 245614436.6233 - val_loss: 1009116421.8082\n",
      "Epoch 591/1000\n",
      "1168/1168 [==============================] - 0s 205us/step - loss: 241273241.8973 - val_loss: 1035835449.7260\n",
      "Epoch 592/1000\n",
      "1168/1168 [==============================] - 0s 233us/step - loss: 254675346.5000 - val_loss: 1123195836.7192\n",
      "Epoch 593/1000\n",
      "1168/1168 [==============================] - 0s 226us/step - loss: 249582827.2945 - val_loss: 979851398.9589\n",
      "Epoch 594/1000\n",
      "1168/1168 [==============================] - 0s 212us/step - loss: 236358392.4315 - val_loss: 957158942.9315\n",
      "Epoch 595/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 267280301.3151 - val_loss: 1086477806.3562\n",
      "Epoch 596/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 260686078.1644 - val_loss: 923130630.3562\n",
      "Epoch 597/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 232899988.1747 - val_loss: 1002821556.9863\n",
      "Epoch 598/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 242662424.9658 - val_loss: 996496895.2329\n",
      "Epoch 599/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 245281655.1096 - val_loss: 972192863.4521\n",
      "Epoch 600/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 237209664.4623 - val_loss: 960479329.3151\n",
      "Epoch 601/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 299120072.4658 - val_loss: 974310998.3562\n",
      "Epoch 602/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 286598113.8048 - val_loss: 961617497.6986\n",
      "Epoch 603/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 218366691.7055 - val_loss: 1009234594.4658\n",
      "Epoch 604/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 237294307.9418 - val_loss: 992164354.6301\n",
      "Epoch 605/1000\n",
      "1168/1168 [==============================] - 1s 918us/step - loss: 241510817.5342 - val_loss: 975395191.8356ETA: 0s - loss: 220973\n",
      "Epoch 606/1000\n",
      "1168/1168 [==============================] - 1s 671us/step - loss: 232306691.7842 - val_loss: 938099572.4932\n",
      "Epoch 607/1000\n",
      "1168/1168 [==============================] - 1s 801us/step - loss: 245631037.0000 - val_loss: 913735294.6027\n",
      "Epoch 608/1000\n",
      "1168/1168 [==============================] - 1s 541us/step - loss: 249356627.6781 - val_loss: 966478292.1644\n",
      "Epoch 609/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 303374218.5205 - val_loss: 966701108.7123\n",
      "Epoch 610/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 300413359.7945 - val_loss: 958223812.4932\n",
      "Epoch 611/1000\n",
      "1168/1168 [==============================] - 1s 534us/step - loss: 232492423.5342 - val_loss: 982013792.8219\n",
      "Epoch 612/1000\n",
      "1168/1168 [==============================] - 1s 562us/step - loss: 230665141.2055 - val_loss: 977451728.7123\n",
      "Epoch 613/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 236101161.5137 - val_loss: 985791945.6712\n",
      "Epoch 614/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 247514237.8493 - val_loss: 994390950.7945\n",
      "Epoch 615/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 252546107.9486 - val_loss: 1027070890.8904\n",
      "Epoch 616/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 235143741.1918 - val_loss: 987018513.9726\n",
      "Epoch 617/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 237608757.1164 - val_loss: 950002456.3836\n",
      "Epoch 618/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 231929794.8116 - val_loss: 867268073.1781\n",
      "Epoch 619/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 250738076.3767 - val_loss: 982902484.6575\n",
      "Epoch 620/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 238644223.9452 - val_loss: 908290421.0411\n",
      "Epoch 621/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 225990847.1438 - val_loss: 955969946.8493\n",
      "Epoch 622/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 232504406.1849 - val_loss: 1012668826.6575\n",
      "Epoch 623/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 229910586.7260 - val_loss: 1081628589.8288\n",
      "Epoch 624/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 222395703.1301 - val_loss: 982950652.6027\n",
      "Epoch 625/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 276740745.4178 - val_loss: 1113063864.0959\n",
      "Epoch 626/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 288239719.7226 - val_loss: 1013516644.9041\n",
      "Epoch 627/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 227825545.7260 - val_loss: 996848671.1781\n",
      "Epoch 628/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 236411354.7397 - val_loss: 1010478491.9452\n",
      "Epoch 629/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 237516830.2740 - val_loss: 905124124.4384\n",
      "Epoch 630/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 248380328.0685 - val_loss: 1009221666.1918\n",
      "Epoch 631/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 236698321.5377 - val_loss: 1011986628.7123\n",
      "Epoch 632/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 248485648.0959 - val_loss: 927561112.3836\n",
      "Epoch 633/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 231333802.3288 - val_loss: 993563614.4658\n",
      "Epoch 634/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 220008856.7192 - val_loss: 977849814.3836\n",
      "Epoch 635/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 211839583.2466 - val_loss: 959346163.1781\n",
      "Epoch 636/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 229204658.2877 - val_loss: 1002990222.4658\n",
      "Epoch 637/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 259119494.5753 - val_loss: 1035063995.3973\n",
      "Epoch 638/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 246975554.0651 - val_loss: 980095435.5068\n",
      "Epoch 639/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 235846403.9247 - val_loss: 989566250.5205\n",
      "Epoch 640/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 241205730.0890 - val_loss: 895205019.2877\n",
      "Epoch 641/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 214752595.7603 - val_loss: 951989752.1644\n",
      "Epoch 642/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 213658237.5753 - val_loss: 1133438200.5274\n",
      "Epoch 643/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 218745772.9932 - val_loss: 943028155.2329\n",
      "Epoch 644/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 242494766.7603 - val_loss: 928281783.3425\n",
      "Epoch 645/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 229465190.2740 - val_loss: 1042486790.6438\n",
      "Epoch 646/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 227971104.9247 - val_loss: 964478950.6849\n",
      "Epoch 647/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 237910886.4521 - val_loss: 892453772.8767\n",
      "Epoch 648/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 222502191.1781 - val_loss: 979654146.9178\n",
      "Epoch 649/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 225757315.8288 - val_loss: 939128133.5890\n",
      "Epoch 650/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 230572181.7397 - val_loss: 958326810.7945\n",
      "Epoch 651/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 241261982.3082 - val_loss: 1034048206.5479\n",
      "Epoch 652/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 214606014.1370 - val_loss: 954825970.9589\n",
      "Epoch 653/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 370us/step - loss: 221065446.7637 - val_loss: 1132235649.9452\n",
      "Epoch 654/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 223032383.6849 - val_loss: 1072742754.3562\n",
      "Epoch 655/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 223848112.5068 - val_loss: 985401210.9041\n",
      "Epoch 656/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 230031689.7534 - val_loss: 984066282.0274\n",
      "Epoch 657/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 231332786.4726 - val_loss: 917525218.7945\n",
      "Epoch 658/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 215469944.9863 - val_loss: 987270489.1507\n",
      "Epoch 659/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 219264430.7466 - val_loss: 977305669.6986\n",
      "Epoch 660/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 239876548.4041 - val_loss: 902215059.8904\n",
      "Epoch 661/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 246640049.8699 - val_loss: 901606309.5068\n",
      "Epoch 662/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 252704951.4212 - val_loss: 905304916.7123\n",
      "Epoch 663/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 209607226.3562 - val_loss: 986453636.8082\n",
      "Epoch 664/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 215202413.4623 - val_loss: 965171607.9452\n",
      "Epoch 665/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 240430216.9144 - val_loss: 1095288445.1507\n",
      "Epoch 666/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 230383752.2055 - val_loss: 995905868.7945\n",
      "Epoch 667/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 251119768.5428 - val_loss: 930232243.2055\n",
      "Epoch 668/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 223717260.5137 - val_loss: 1074214132.4932\n",
      "Epoch 669/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 218919562.3014 - val_loss: 977288125.2055\n",
      "Epoch 670/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 203263791.2877 - val_loss: 992113360.1096\n",
      "Epoch 671/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 217900047.5274 - val_loss: 1010969920.0137\n",
      "Epoch 672/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 244917169.1781 - val_loss: 865674612.7123\n",
      "Epoch 673/1000\n",
      "1168/1168 [==============================] - 1s 664us/step - loss: 243574912.4452 - val_loss: 1017739715.1644\n",
      "Epoch 674/1000\n",
      "1168/1168 [==============================] - 1s 657us/step - loss: 213899866.0205 - val_loss: 898884015.9178\n",
      "Epoch 675/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 237345866.9110 - val_loss: 1239594421.0959\n",
      "Epoch 676/1000\n",
      "1168/1168 [==============================] - 1s 664us/step - loss: 213494627.3664 - val_loss: 976956172.1096\n",
      "Epoch 677/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 209409159.8219 - val_loss: 918871109.3699\n",
      "Epoch 678/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 215667368.8288 - val_loss: 915202561.4247\n",
      "Epoch 679/1000\n",
      "1168/1168 [==============================] - 1s 657us/step - loss: 203363406.4178 - val_loss: 957283126.7945\n",
      "Epoch 680/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 232911071.8253 - val_loss: 963091074.4658\n",
      "Epoch 681/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 215777241.5548 - val_loss: 933329660.7945\n",
      "Epoch 682/1000\n",
      "1168/1168 [==============================] - 1s 692us/step - loss: 195061238.6986 - val_loss: 988996224.9452\n",
      "Epoch 683/1000\n",
      "1168/1168 [==============================] - 1s 692us/step - loss: 235342470.2089 - val_loss: 962311414.8493\n",
      "Epoch 684/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 214782490.9589 - val_loss: 929287768.5205\n",
      "Epoch 685/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 203722787.8630 - val_loss: 895036202.1370\n",
      "Epoch 686/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 232046378.0856 - val_loss: 1274849872.6849\n",
      "Epoch 687/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 234800453.6027 - val_loss: 958998997.5890\n",
      "Epoch 688/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 230906617.1370 - val_loss: 1075920232.7123\n",
      "Epoch 689/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 216620335.4795 - val_loss: 1030852328.4384\n",
      "Epoch 690/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 223764904.2774 - val_loss: 975043267.1644\n",
      "Epoch 691/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 206681773.0479 - val_loss: 965830589.9178\n",
      "Epoch 692/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 205914313.8493 - val_loss: 958790547.6164\n",
      "Epoch 693/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 200612296.8836 - val_loss: 925934353.0959\n",
      "Epoch 694/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 241266088.8938 - val_loss: 1031930396.3288\n",
      "Epoch 695/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 216823274.8767 - val_loss: 946839801.1507\n",
      "Epoch 696/1000\n",
      "1168/1168 [==============================] - 1s 500us/step - loss: 224308750.8630 - val_loss: 940541669.6712\n",
      "Epoch 697/1000\n",
      "1168/1168 [==============================] - 1s 493us/step - loss: 238429812.1918 - val_loss: 918304494.6849\n",
      "Epoch 698/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 203442593.6130 - val_loss: 969163187.3425\n",
      "Epoch 699/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 201719039.0205 - val_loss: 1098540348.4110\n",
      "Epoch 700/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 209217277.1113 - val_loss: 962831029.4110\n",
      "Epoch 701/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 218295905.2329 - val_loss: 864335945.6438\n",
      "Epoch 702/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 218644105.8973 - val_loss: 837754304.4384\n",
      "Epoch 703/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 211258627.7380 - val_loss: 915539070.9041\n",
      "Epoch 704/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 211416555.8630 - val_loss: 933407621.2603\n",
      "Epoch 705/1000\n",
      "1168/1168 [==============================] - 1s 480us/step - loss: 204705666.2534 - val_loss: 1007441314.8356\n",
      "Epoch 706/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 198426012.9726 - val_loss: 998668539.4795\n",
      "Epoch 707/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 222713747.4795 - val_loss: 903122726.7945\n",
      "Epoch 708/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 214891297.5890 - val_loss: 1010682105.4247\n",
      "Epoch 709/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 217613714.9075 - val_loss: 905879100.5753\n",
      "Epoch 710/1000\n",
      "1168/1168 [==============================] - 1s 596us/step - loss: 224242558.2329 - val_loss: 1000058921.6986\n",
      "Epoch 711/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 235351240.3493 - val_loss: 980109555.8356\n",
      "Epoch 712/1000\n",
      "1168/1168 [==============================] - 1s 541us/step - loss: 207527624.4247 - val_loss: 987557694.9452\n",
      "Epoch 713/1000\n",
      "1168/1168 [==============================] - 1s 534us/step - loss: 204880837.1918 - val_loss: 1262303727.2329\n",
      "Epoch 714/1000\n",
      "1168/1168 [==============================] - 1s 548us/step - loss: 238989262.7842 - val_loss: 1085791524.5753\n",
      "Epoch 715/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 225342807.4007 - val_loss: 918241439.6712\n",
      "Epoch 716/1000\n",
      "1168/1168 [==============================] - 1s 534us/step - loss: 219654868.0000 - val_loss: 971099702.1781\n",
      "Epoch 717/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 239456428.3185 - val_loss: 1060390918.6301\n",
      "Epoch 718/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 242564191.8425 - val_loss: 867178987.5616\n",
      "Epoch 719/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 1s 479us/step - loss: 199865973.5205 - val_loss: 957859494.9589\n",
      "Epoch 720/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 202153145.2911 - val_loss: 906060826.0274\n",
      "Epoch 721/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 223490989.6370 - val_loss: 902820115.4521\n",
      "Epoch 722/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 198902693.8151 - val_loss: 913870636.1644\n",
      "Epoch 723/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 214855887.4281 - val_loss: 919142842.9041\n",
      "Epoch 724/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 212563291.4829 - val_loss: 937620447.7808\n",
      "Epoch 725/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 212697679.6986 - val_loss: 916688180.3151\n",
      "Epoch 726/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 204518658.4384 - val_loss: 1027181782.8493\n",
      "Epoch 727/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 192940380.2740 - val_loss: 1004619271.0137\n",
      "Epoch 728/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 200110337.8938 - val_loss: 891611711.6849\n",
      "Epoch 729/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 238768623.7466 - val_loss: 908224512.1644\n",
      "Epoch 730/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 203356657.1866 - val_loss: 970209332.6027\n",
      "Epoch 731/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 199612353.3288 - val_loss: 978146824.9315\n",
      "Epoch 732/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 226231710.3082 - val_loss: 910456665.5342\n",
      "Epoch 733/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 206595624.5616 - val_loss: 995921562.8082\n",
      "Epoch 734/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 199692066.3356 - val_loss: 957998418.1370\n",
      "Epoch 735/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 231564479.0479 - val_loss: 898412605.3699\n",
      "Epoch 736/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 204356795.3836 - val_loss: 957624323.8356\n",
      "Epoch 737/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 193727966.2603 - val_loss: 1002901674.2260\n",
      "Epoch 738/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 208251889.9041 - val_loss: 934484775.6712\n",
      "Epoch 739/1000\n",
      "1168/1168 [==============================] - 1s 480us/step - loss: 201090701.0959 - val_loss: 994087404.2740\n",
      "Epoch 740/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 198666337.5685 - val_loss: 997760989.6712\n",
      "Epoch 741/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 227414647.5068 - val_loss: 921655122.6027\n",
      "Epoch 742/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 228619730.5548 - val_loss: 846751620.4932\n",
      "Epoch 743/1000\n",
      "1168/1168 [==============================] - 1s 774us/step - loss: 205770160.2671 - val_loss: 1009516518.5753\n",
      "Epoch 744/1000\n",
      "1168/1168 [==============================] - 1s 849us/step - loss: 218140742.6575 - val_loss: 1042814391.3151\n",
      "Epoch 745/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 202493455.8767 - val_loss: 876951506.9041\n",
      "Epoch 746/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 196222863.9418 - val_loss: 1000914472.4110\n",
      "Epoch 747/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 219320811.3699 - val_loss: 934622876.1644\n",
      "Epoch 748/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 220639875.8973 - val_loss: 962208175.2055\n",
      "Epoch 749/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 207718816.2877 - val_loss: 1090627957.1507\n",
      "Epoch 750/1000\n",
      "1168/1168 [==============================] - 0s 349us/step - loss: 189468560.9144 - val_loss: 905157248.8493\n",
      "Epoch 751/1000\n",
      "1168/1168 [==============================] - 0s 336us/step - loss: 192920411.4075 - val_loss: 862521776.8767\n",
      "Epoch 752/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 202303116.4178 - val_loss: 987172399.5616\n",
      "Epoch 753/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 193553248.6404 - val_loss: 1032595817.6438\n",
      "Epoch 754/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 194744276.0205 - val_loss: 1019181959.6986\n",
      "Epoch 755/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 220854334.6678 - val_loss: 931517874.9589\n",
      "Epoch 756/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 193243259.8048 - val_loss: 996085058.6575\n",
      "Epoch 757/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 193380119.3151 - val_loss: 951925685.3562\n",
      "Epoch 758/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 219637454.3356 - val_loss: 1166808821.5068\n",
      "Epoch 759/1000\n",
      "1168/1168 [==============================] - 1s 614us/step - loss: 205186376.4384 - val_loss: 847350094.3836\n",
      "Epoch 760/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 202838990.9178 - val_loss: 936355233.5616\n",
      "Epoch 761/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 210093833.3425 - val_loss: 964775153.3151\n",
      "Epoch 762/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 235378277.3253 - val_loss: 1127003046.1918\n",
      "Epoch 763/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 227862902.1267 - val_loss: 907996544.1096\n",
      "Epoch 764/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 209886970.2945 - val_loss: 891746797.9178\n",
      "Epoch 765/1000\n",
      "1168/1168 [==============================] - 1s 644us/step - loss: 210491980.8973 - val_loss: 927136723.6986\n",
      "Epoch 766/1000\n",
      "1168/1168 [==============================] - 1s 801us/step - loss: 208835302.3219 - val_loss: 1210740138.0822\n",
      "Epoch 767/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 216323515.2740 - val_loss: 880213350.9315\n",
      "Epoch 768/1000\n",
      "1168/1168 [==============================] - 1s 657us/step - loss: 202252514.6387 - val_loss: 987140399.3425\n",
      "Epoch 769/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 200562616.6712 - val_loss: 890665237.5342\n",
      "Epoch 770/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 208668694.1438 - val_loss: 938272186.1918\n",
      "Epoch 771/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 190820594.1986 - val_loss: 969646106.4795\n",
      "Epoch 772/1000\n",
      "1168/1168 [==============================] - 0s 418us/step - loss: 184615418.0411 - val_loss: 920381486.1370\n",
      "Epoch 773/1000\n",
      "1168/1168 [==============================] - 0s 383us/step - loss: 203026896.2329 - val_loss: 929903143.7260\n",
      "Epoch 774/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 203925230.3527 - val_loss: 1110666624.1918\n",
      "Epoch 775/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 218943526.0411 - val_loss: 942988108.6849\n",
      "Epoch 776/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 209248189.7808 - val_loss: 890275220.1096\n",
      "Epoch 777/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 181511738.9863 - val_loss: 1023327628.1918\n",
      "Epoch 778/1000\n",
      "1168/1168 [==============================] - 0s 384us/step - loss: 190233337.3630 - val_loss: 951966464.9863\n",
      "Epoch 779/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 201431110.9726 - val_loss: 942680094.8493\n",
      "Epoch 780/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 199579077.4384 - val_loss: 946630108.3836\n",
      "Epoch 781/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 197385173.3973 - val_loss: 886133745.0137\n",
      "Epoch 782/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 231451951.3973 - val_loss: 825779395.0685\n",
      "Epoch 783/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 208398801.8219 - val_loss: 1030483250.9589\n",
      "Epoch 784/1000\n",
      "1168/1168 [==============================] - 0s 390us/step - loss: 202795059.2568 - val_loss: 922803633.5342\n",
      "Epoch 785/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 384us/step - loss: 195545701.4589 - val_loss: 1017016915.5616\n",
      "Epoch 786/1000\n",
      "1168/1168 [==============================] - 0s 370us/step - loss: 195210744.7089 - val_loss: 909808262.2466\n",
      "Epoch 787/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 181188984.0548 - val_loss: 910266954.5205\n",
      "Epoch 788/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 188003359.5959 - val_loss: 947911420.4658\n",
      "Epoch 789/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 191520766.7158 - val_loss: 1083531237.6438\n",
      "Epoch 790/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 197852645.6301 - val_loss: 903436559.3699\n",
      "Epoch 791/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 200824224.3699 - val_loss: 1008701022.2466\n",
      "Epoch 792/1000\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 184269656.1130 - val_loss: 878110273.9452\n",
      "Epoch 793/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 189845941.0788 - val_loss: 923007489.7534\n",
      "Epoch 794/1000\n",
      "1168/1168 [==============================] - 1s 692us/step - loss: 192298839.1370 - val_loss: 983677274.5753\n",
      "Epoch 795/1000\n",
      "1168/1168 [==============================] - 1s 747us/step - loss: 175656893.8836 - val_loss: 869267865.8356\n",
      "Epoch 796/1000\n",
      "1168/1168 [==============================] - 1s 671us/step - loss: 185167572.9486 - val_loss: 913357728.0822\n",
      "Epoch 797/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 192099492.4555 - val_loss: 948847809.6438\n",
      "Epoch 798/1000\n",
      "1168/1168 [==============================] - 1s 562us/step - loss: 384548101.8425 - val_loss: 931055632.9863\n",
      "Epoch 799/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 206749786.7808 - val_loss: 892800069.1644\n",
      "Epoch 800/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 212469617.5205 - val_loss: 893196648.1096\n",
      "Epoch 801/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 180882347.2671 - val_loss: 958324263.5068\n",
      "Epoch 802/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 189344584.5205 - val_loss: 943332531.5342\n",
      "Epoch 803/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 203124476.6370 - val_loss: 885830704.1096\n",
      "Epoch 804/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 193403463.4366 - val_loss: 878302304.0548\n",
      "Epoch 805/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 189078828.9007 - val_loss: 902974657.0411\n",
      "Epoch 806/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 196709138.2705 - val_loss: 1114395717.0959\n",
      "Epoch 807/1000\n",
      "1168/1168 [==============================] - 1s 521us/step - loss: 177288759.8973 - val_loss: 915733186.1644\n",
      "Epoch 808/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 181693433.5274 - val_loss: 1103004425.6986\n",
      "Epoch 809/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 196066800.5342 - val_loss: 838847257.5068\n",
      "Epoch 810/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 198613215.3014 - val_loss: 973070369.3699\n",
      "Epoch 811/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 198807144.0411 - val_loss: 882364314.0274\n",
      "Epoch 812/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 181574812.4144 - val_loss: 859805210.0822\n",
      "Epoch 813/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 206215277.2158 - val_loss: 951699676.2740\n",
      "Epoch 814/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 203567284.3562 - val_loss: 846406465.6164\n",
      "Epoch 815/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 183719811.5925 - val_loss: 917135612.0548\n",
      "Epoch 816/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 236289742.0959 - val_loss: 894610525.9178\n",
      "Epoch 817/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 250351768.7055 - val_loss: 1465850550.6849\n",
      "Epoch 818/1000\n",
      "1168/1168 [==============================] - 1s 740us/step - loss: 203888902.6969 - val_loss: 921673684.2466\n",
      "Epoch 819/1000\n",
      "1168/1168 [==============================] - 1s 767us/step - loss: 173150080.1918 - val_loss: 873488210.7397\n",
      "Epoch 820/1000\n",
      "1168/1168 [==============================] - 1s 644us/step - loss: 190336168.0753 - val_loss: 893735832.9589\n",
      "Epoch 821/1000\n",
      "1168/1168 [==============================] - 1s 541us/step - loss: 224064556.9726 - val_loss: 970097416.5205\n",
      "Epoch 822/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 184241748.3767 - val_loss: 866113359.5342\n",
      "Epoch 823/1000\n",
      "1168/1168 [==============================] - 0s 418us/step - loss: 200695394.0651 - val_loss: 943090552.2192\n",
      "Epoch 824/1000\n",
      "1168/1168 [==============================] - 0s 356us/step - loss: 215016170.9932 - val_loss: 879606017.6986\n",
      "Epoch 825/1000\n",
      "1168/1168 [==============================] - 0s 329us/step - loss: 186959444.5034 - val_loss: 872045065.5616\n",
      "Epoch 826/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 186908055.3185 - val_loss: 1001830781.5342\n",
      "Epoch 827/1000\n",
      "1168/1168 [==============================] - 0s 274us/step - loss: 185902920.6062 - val_loss: 921829221.9178\n",
      "Epoch 828/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 198584043.0993 - val_loss: 932179680.1644\n",
      "Epoch 829/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 186406508.8116 - val_loss: 921730374.0274\n",
      "Epoch 830/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 166528100.7534 - val_loss: 948993636.7671\n",
      "Epoch 831/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 183603769.4435 - val_loss: 921237720.3562\n",
      "Epoch 832/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 179422713.7192 - val_loss: 864188072.5479\n",
      "Epoch 833/1000\n",
      "1168/1168 [==============================] - 1s 801us/step - loss: 186607974.7021 - val_loss: 994874174.7945\n",
      "Epoch 834/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 183903413.4349 - val_loss: 992366896.4110\n",
      "Epoch 835/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 184411684.4572 - val_loss: 796283606.8767\n",
      "Epoch 836/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 173729811.7877 - val_loss: 987627022.9315\n",
      "Epoch 837/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 236948492.0308 - val_loss: 860954973.9178\n",
      "Epoch 838/1000\n",
      "1168/1168 [==============================] - 1s 562us/step - loss: 185497694.5668 - val_loss: 846665579.9452\n",
      "Epoch 839/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 177213657.6438 - val_loss: 896941856.3836\n",
      "Epoch 840/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 184260313.5719 - val_loss: 820762311.8904\n",
      "Epoch 841/1000\n",
      "1168/1168 [==============================] - 1s 562us/step - loss: 187293466.9349 - val_loss: 917133847.2329\n",
      "Epoch 842/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 179746270.9555 - val_loss: 965950071.5342\n",
      "Epoch 843/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 185795662.1781 - val_loss: 1038248555.9452\n",
      "Epoch 844/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 183323029.6952 - val_loss: 893288667.0959\n",
      "Epoch 845/1000\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 197003850.7226 - val_loss: 987337140.1644\n",
      "Epoch 846/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 188482364.6986 - val_loss: 1008050121.2055\n",
      "Epoch 847/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 205807182.7500 - val_loss: 1029182183.6164\n",
      "Epoch 848/1000\n",
      "1168/1168 [==============================] - 1s 431us/step - loss: 204185619.3390 - val_loss: 1066337854.4110\n",
      "Epoch 849/1000\n",
      "1168/1168 [==============================] - 0s 397us/step - loss: 206036962.4932 - val_loss: 900977308.8219\n",
      "Epoch 850/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 191715078.1438 - val_loss: 908217649.4247\n",
      "Epoch 851/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 336us/step - loss: 187644840.8493 - val_loss: 875554035.3425\n",
      "Epoch 852/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 173636242.1644 - val_loss: 883241819.6438\n",
      "Epoch 853/1000\n",
      "1168/1168 [==============================] - 0s 274us/step - loss: 174791913.9760 - val_loss: 857633715.0959\n",
      "Epoch 854/1000\n",
      "1168/1168 [==============================] - 0s 219us/step - loss: 198717499.2432 - val_loss: 938543238.5479\n",
      "Epoch 855/1000\n",
      "1168/1168 [==============================] - 1s 562us/step - loss: 166177987.0685 - val_loss: 872092046.5753\n",
      "Epoch 856/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 192967819.8185 - val_loss: 849817475.6164\n",
      "Epoch 857/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 186386242.3459 - val_loss: 918433470.8493\n",
      "Epoch 858/1000\n",
      "1168/1168 [==============================] - 1s 623us/step - loss: 179699357.9452 - val_loss: 916563193.2055\n",
      "Epoch 859/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 188376725.7466 - val_loss: 967184404.7671\n",
      "Epoch 860/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 195943420.8767 - val_loss: 951817534.5205\n",
      "Epoch 861/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 192584146.2945 - val_loss: 848632011.8356\n",
      "Epoch 862/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 183770290.8031 - val_loss: 885174221.4247\n",
      "Epoch 863/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 170453041.0514 - val_loss: 902156268.8767\n",
      "Epoch 864/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 178573011.9212 - val_loss: 932694603.6164\n",
      "Epoch 865/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 177288638.4658 - val_loss: 1025090864.0548\n",
      "Epoch 866/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 196445928.5377 - val_loss: 865443602.4110\n",
      "Epoch 867/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 179073897.6096 - val_loss: 889818005.5342\n",
      "Epoch 868/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 178803147.8938 - val_loss: 884446857.2055\n",
      "Epoch 869/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 204734119.4966 - val_loss: 855222874.4521\n",
      "Epoch 870/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 194374373.5822 - val_loss: 859076470.9863\n",
      "Epoch 871/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 167674811.8596 - val_loss: 904363121.5616\n",
      "Epoch 872/1000\n",
      "1168/1168 [==============================] - 1s 589us/step - loss: 193574396.0668 - val_loss: 892015726.1918\n",
      "Epoch 873/1000\n",
      "1168/1168 [==============================] - 1s 959us/step - loss: 194653124.8973 - val_loss: 907847302.3014\n",
      "Epoch 874/1000\n",
      "1168/1168 [==============================] - 1s 671us/step - loss: 179404221.3596 - val_loss: 1001693690.6849\n",
      "Epoch 875/1000\n",
      "1168/1168 [==============================] - 1s 616us/step - loss: 193811860.2671 - val_loss: 930443518.4932\n",
      "Epoch 876/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 192750946.1644 - val_loss: 866209956.9863\n",
      "Epoch 877/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 187690962.0959 - val_loss: 842921033.8356\n",
      "Epoch 878/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 169635810.5274 - val_loss: 916833288.5205\n",
      "Epoch 879/1000\n",
      "1168/1168 [==============================] - 1s 541us/step - loss: 193526654.0068 - val_loss: 881931996.9863\n",
      "Epoch 880/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 176530012.1815 - val_loss: 999311593.1507\n",
      "Epoch 881/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 175759043.0616 - val_loss: 968273624.9041\n",
      "Epoch 882/1000\n",
      "1168/1168 [==============================] - 1s 521us/step - loss: 194014257.8904 - val_loss: 831901756.6027\n",
      "Epoch 883/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 201538446.2192 - val_loss: 998220910.5205\n",
      "Epoch 884/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 174149181.1130 - val_loss: 925640229.3562\n",
      "Epoch 885/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 167565431.3870 - val_loss: 904754308.8219\n",
      "Epoch 886/1000\n",
      "1168/1168 [==============================] - 1s 520us/step - loss: 181228736.2055 - val_loss: 893749175.6986\n",
      "Epoch 887/1000\n",
      "1168/1168 [==============================] - 1s 521us/step - loss: 169183110.6404 - val_loss: 833883586.9041\n",
      "Epoch 888/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 187696190.4726 - val_loss: 872064432.5479\n",
      "Epoch 889/1000\n",
      "1168/1168 [==============================] - 1s 548us/step - loss: 182083851.1986 - val_loss: 971165207.7808\n",
      "Epoch 890/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 184402234.8767 - val_loss: 997679908.9315\n",
      "Epoch 891/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 201240359.6404 - val_loss: 939358087.7260\n",
      "Epoch 892/1000\n",
      "1168/1168 [==============================] - 1s 521us/step - loss: 207798349.0616 - val_loss: 966265218.0822\n",
      "Epoch 893/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 177132403.6712 - val_loss: 906915600.4932\n",
      "Epoch 894/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 192664852.0993 - val_loss: 831519349.0959\n",
      "Epoch 895/1000\n",
      "1168/1168 [==============================] - 1s 555us/step - loss: 174618620.3562 - val_loss: 969440218.5753\n",
      "Epoch 896/1000\n",
      "1168/1168 [==============================] - 1s 548us/step - loss: 172344336.5342 - val_loss: 872056072.6301\n",
      "Epoch 897/1000\n",
      "1168/1168 [==============================] - 1s 671us/step - loss: 166291744.4829 - val_loss: 921761022.4658\n",
      "Epoch 898/1000\n",
      "1168/1168 [==============================] - 1s 541us/step - loss: 181006327.3767 - val_loss: 950629778.8493\n",
      "Epoch 899/1000\n",
      "1168/1168 [==============================] - 1s 473us/step - loss: 182228663.9384 - val_loss: 854147447.6164\n",
      "Epoch 900/1000\n",
      "1168/1168 [==============================] - 1s 438us/step - loss: 171683620.9521 - val_loss: 863798690.6849\n",
      "Epoch 901/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 187464482.9863 - val_loss: 934192625.5342\n",
      "Epoch 902/1000\n",
      "1168/1168 [==============================] - 0s 377us/step - loss: 164067954.3219 - val_loss: 960195255.0822\n",
      "Epoch 903/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 169236150.7089 - val_loss: 840180754.0822\n",
      "Epoch 904/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 178880169.5411 - val_loss: 886886467.0685\n",
      "Epoch 905/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 173927387.5274 - val_loss: 844084898.6301\n",
      "Epoch 906/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 166981948.5171 - val_loss: 925773653.6027\n",
      "Epoch 907/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 170258729.4195 - val_loss: 1047116900.2466\n",
      "Epoch 908/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 184104628.1130 - val_loss: 896709949.3425\n",
      "Epoch 909/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 205676278.9315 - val_loss: 940800336.2466\n",
      "Epoch 910/1000\n",
      "1168/1168 [==============================] - 0s 322us/step - loss: 195152414.5274 - val_loss: 951885522.7945\n",
      "Epoch 911/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 167681148.6575 - val_loss: 825470876.7123\n",
      "Epoch 912/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 169697294.3219 - val_loss: 875434247.3699\n",
      "Epoch 913/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 198860112.2740 - val_loss: 883112701.8082\n",
      "Epoch 914/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 178342709.5342 - val_loss: 871327339.5616\n",
      "Epoch 915/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 196829387.9247 - val_loss: 977422829.2329\n",
      "Epoch 916/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 204181933.7568 - val_loss: 840310579.3973\n",
      "Epoch 917/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 301us/step - loss: 185635193.5308 - val_loss: 889892420.3836\n",
      "Epoch 918/1000\n",
      "1168/1168 [==============================] - 0s 315us/step - loss: 167973155.6473 - val_loss: 958378010.1370\n",
      "Epoch 919/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 163243097.3185 - val_loss: 878195708.2740\n",
      "Epoch 920/1000\n",
      "1168/1168 [==============================] - 1s 639us/step - loss: 165415841.9144 - val_loss: 844007782.1918\n",
      "Epoch 921/1000\n",
      "1168/1168 [==============================] - 1s 582us/step - loss: 170937932.8527 - val_loss: 913580418.5753\n",
      "Epoch 922/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 166364738.6267 - val_loss: 776697898.3562\n",
      "Epoch 923/1000\n",
      "1168/1168 [==============================] - 1s 610us/step - loss: 190826531.5719 - val_loss: 910207933.4521\n",
      "Epoch 924/1000\n",
      "1168/1168 [==============================] - 1s 657us/step - loss: 177897030.3322 - val_loss: 948934055.3973\n",
      "Epoch 925/1000\n",
      "1168/1168 [==============================] - 1s 664us/step - loss: 178132145.3322 - val_loss: 832902704.4384\n",
      "Epoch 926/1000\n",
      "1168/1168 [==============================] - 1s 747us/step - loss: 174182853.3955 - val_loss: 896283788.9863\n",
      "Epoch 927/1000\n",
      "1168/1168 [==============================] - 1s 774us/step - loss: 184900381.9007 - val_loss: 881325825.8082\n",
      "Epoch 928/1000\n",
      "1168/1168 [==============================] - 1s 815us/step - loss: 155851402.5685 - val_loss: 883347392.6575\n",
      "Epoch 929/1000\n",
      "1168/1168 [==============================] - 1s 664us/step - loss: 173486487.5377 - val_loss: 863124435.2877\n",
      "Epoch 930/1000\n",
      "1168/1168 [==============================] - 1s 527us/step - loss: 187867828.6678 - val_loss: 907298940.5205\n",
      "Epoch 931/1000\n",
      "1168/1168 [==============================] - 1s 500us/step - loss: 167009757.0171 - val_loss: 848515873.0685\n",
      "Epoch 932/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 190856086.9110 - val_loss: 877302621.8356\n",
      "Epoch 933/1000\n",
      "1168/1168 [==============================] - 0s 418us/step - loss: 162710243.7979 - val_loss: 862697374.6849\n",
      "Epoch 934/1000\n",
      "1168/1168 [==============================] - 0s 404us/step - loss: 183892721.4726 - val_loss: 992058105.6712\n",
      "Epoch 935/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 161662531.3836 - val_loss: 831605361.3699\n",
      "Epoch 936/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 181072174.6610 - val_loss: 953497091.1233\n",
      "Epoch 937/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 161324001.2945 - val_loss: 968504648.2740\n",
      "Epoch 938/1000\n",
      "1168/1168 [==============================] - 0s 267us/step - loss: 167206468.6678 - val_loss: 950789663.8904\n",
      "Epoch 939/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 171766989.7243 - val_loss: 964438233.2877\n",
      "Epoch 940/1000\n",
      "1168/1168 [==============================] - 0s 342us/step - loss: 166337265.7466 - val_loss: 831041720.3836\n",
      "Epoch 941/1000\n",
      "1168/1168 [==============================] - 0s 294us/step - loss: 169121341.8425 - val_loss: 859820435.2329\n",
      "Epoch 942/1000\n",
      "1168/1168 [==============================] - 0s 288us/step - loss: 163155721.1027 - val_loss: 852920442.9041\n",
      "Epoch 943/1000\n",
      "1168/1168 [==============================] - 0s 240us/step - loss: 266568430.6370 - val_loss: 990672781.3699\n",
      "Epoch 944/1000\n",
      "1168/1168 [==============================] - 0s 260us/step - loss: 229538795.3048 - val_loss: 872779325.2877\n",
      "Epoch 945/1000\n",
      "1168/1168 [==============================] - 0s 233us/step - loss: 183227890.2740 - val_loss: 920532809.4795\n",
      "Epoch 946/1000\n",
      "1168/1168 [==============================] - 0s 295us/step - loss: 155458905.2928 - val_loss: 910694404.8493\n",
      "Epoch 947/1000\n",
      "1168/1168 [==============================] - 1s 479us/step - loss: 164417057.7432 - val_loss: 879183561.3699\n",
      "Epoch 948/1000\n",
      "1168/1168 [==============================] - 1s 801us/step - loss: 162993296.7808 - val_loss: 872922577.6438\n",
      "Epoch 949/1000\n",
      "1168/1168 [==============================] - 1s 1ms/step - loss: 162135224.1438 - val_loss: 926138694.2466\n",
      "Epoch 950/1000\n",
      "1168/1168 [==============================] - 1s 705us/step - loss: 163625963.8425 - val_loss: 882627034.5205\n",
      "Epoch 951/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 164456272.0000 - val_loss: 929976823.0137\n",
      "Epoch 952/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 173784217.6473 - val_loss: 919450674.1644\n",
      "Epoch 953/1000\n",
      "1168/1168 [==============================] - 1s 500us/step - loss: 150143363.0274 - val_loss: 910256413.6712\n",
      "Epoch 954/1000\n",
      "1168/1168 [==============================] - 1s 534us/step - loss: 199159296.4401 - val_loss: 924211165.0411\n",
      "Epoch 955/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 158233312.4795 - val_loss: 1031744379.6164\n",
      "Epoch 956/1000\n",
      "1168/1168 [==============================] - 1s 822us/step - loss: 180501798.2123 - val_loss: 957427346.9863\n",
      "Epoch 957/1000\n",
      "1168/1168 [==============================] - 1s 993us/step - loss: 169829343.1781 - val_loss: 887075136.8493\n",
      "Epoch 958/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 163392610.4452 - val_loss: 928040693.2055\n",
      "Epoch 959/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 152789013.3699 - val_loss: 811858128.0000\n",
      "Epoch 960/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 158093187.8493 - val_loss: 911705801.6164\n",
      "Epoch 961/1000\n",
      "1168/1168 [==============================] - 1s 451us/step - loss: 187139162.7226 - val_loss: 880740323.9178\n",
      "Epoch 962/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 189664711.4075 - val_loss: 861507919.8356\n",
      "Epoch 963/1000\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 155435173.2620 - val_loss: 943574253.5616\n",
      "Epoch 964/1000\n",
      "1168/1168 [==============================] - 0s 308us/step - loss: 156014590.1866 - val_loss: 850779159.0137\n",
      "Epoch 965/1000\n",
      "1168/1168 [==============================] - 1s 493us/step - loss: 182097481.0685 - val_loss: 928528908.5753\n",
      "Epoch 966/1000\n",
      "1168/1168 [==============================] - 1s 493us/step - loss: 153631844.1781 - val_loss: 875522277.3151\n",
      "Epoch 967/1000\n",
      "1168/1168 [==============================] - 1s 493us/step - loss: 165768196.7705 - val_loss: 994372854.0274\n",
      "Epoch 968/1000\n",
      "1168/1168 [==============================] - 1s 507us/step - loss: 184794517.4041 - val_loss: 886375128.4384\n",
      "Epoch 969/1000\n",
      "1168/1168 [==============================] - 1s 493us/step - loss: 163442859.7808 - val_loss: 896265381.9178\n",
      "Epoch 970/1000\n",
      "1168/1168 [==============================] - 1s 502us/step - loss: 173151243.5479 - val_loss: 872623404.5205\n",
      "Epoch 971/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 172782020.8390 - val_loss: 944296591.5068\n",
      "Epoch 972/1000\n",
      "1168/1168 [==============================] - 1s 493us/step - loss: 154698518.1712 - val_loss: 948208958.3014\n",
      "Epoch 973/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 147531126.0188 - val_loss: 870879137.1781\n",
      "Epoch 974/1000\n",
      "1168/1168 [==============================] - 1s 493us/step - loss: 150554472.5668 - val_loss: 948657793.7260\n",
      "Epoch 975/1000\n",
      "1168/1168 [==============================] - 1s 486us/step - loss: 156639737.3253 - val_loss: 801987922.7945\n",
      "Epoch 976/1000\n",
      "1168/1168 [==============================] - 1s 493us/step - loss: 157586675.8836 - val_loss: 910577229.1507\n",
      "Epoch 977/1000\n",
      "1168/1168 [==============================] - 1s 541us/step - loss: 170676114.8699 - val_loss: 969866596.9863\n",
      "Epoch 978/1000\n",
      "1168/1168 [==============================] - 1s 603us/step - loss: 153140114.3767 - val_loss: 899291437.5890\n",
      "Epoch 979/1000\n",
      "1168/1168 [==============================] - 1s 767us/step - loss: 153391750.0445 - val_loss: 845972728.2740\n",
      "Epoch 980/1000\n",
      "1168/1168 [==============================] - 1s 1ms/step - loss: 191134895.7945 - val_loss: 981284119.2329\n",
      "Epoch 981/1000\n",
      "1168/1168 [==============================] - 1s 829us/step - loss: 173837083.8356 - val_loss: 936873196.4932\n",
      "Epoch 982/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 195981840.7295 - val_loss: 954279564.8219\n",
      "Epoch 983/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 1s 596us/step - loss: 166017411.4281 - val_loss: 989969945.2055\n",
      "Epoch 984/1000\n",
      "1168/1168 [==============================] - 1s 575us/step - loss: 172967488.5411 - val_loss: 873828760.6849\n",
      "Epoch 985/1000\n",
      "1168/1168 [==============================] - 1s 438us/step - loss: 154286559.1199 - val_loss: 958909374.2603\n",
      "Epoch 986/1000\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 162273468.8527 - val_loss: 945154141.5890\n",
      "Epoch 987/1000\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 180209104.6507 - val_loss: 856357467.1233\n",
      "Epoch 988/1000\n",
      "1168/1168 [==============================] - 0s 411us/step - loss: 183463672.6404 - val_loss: 885371850.6301\n",
      "Epoch 989/1000\n",
      "1168/1168 [==============================] - 0s 363us/step - loss: 187870185.8459 - val_loss: 1167436953.5890\n",
      "Epoch 990/1000\n",
      "1168/1168 [==============================] - 0s 301us/step - loss: 170470926.3116 - val_loss: 890424318.6849\n",
      "Epoch 991/1000\n",
      "1168/1168 [==============================] - 1s 651us/step - loss: 156246242.3390 - val_loss: 898702516.2192\n",
      "Epoch 992/1000\n",
      "1168/1168 [==============================] - 1s 568us/step - loss: 161274843.5137 - val_loss: 992173697.8082\n",
      "Epoch 993/1000\n",
      "1168/1168 [==============================] - 1s 634us/step - loss: 151883918.7123 - val_loss: 1049405954.6849\n",
      "Epoch 994/1000\n",
      "1168/1168 [==============================] - 1s 657us/step - loss: 154922893.2397 - val_loss: 1021552518.4795\n",
      "Epoch 995/1000\n",
      "1168/1168 [==============================] - 1s 630us/step - loss: 152537959.8733 - val_loss: 902479340.8767\n",
      "Epoch 996/1000\n",
      "1168/1168 [==============================] - 1s 678us/step - loss: 166940044.6884 - val_loss: 1029104956.2740\n",
      "Epoch 997/1000\n",
      "1168/1168 [==============================] - 1s 644us/step - loss: 164050371.6062 - val_loss: 925910267.4521\n",
      "Epoch 998/1000\n",
      "1168/1168 [==============================] - 1s 664us/step - loss: 163923776.9178 - val_loss: 850495011.2877\n",
      "Epoch 999/1000\n",
      "1168/1168 [==============================] - 1s 637us/step - loss: 178458030.9418 - val_loss: 1006220952.7123\n",
      "Epoch 1000/1000\n",
      "1168/1168 [==============================] - 1s 571us/step - loss: 150697711.6986 - val_loss: 980108489.0959\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 323))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(model_history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2079636db88>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_prediction = classifier.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_prediction=ANN_prediction.reshape(1459,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id= pd.read_csv('test.csv')['Id']\n",
    "ANN_submission = pd.DataFrame({'Id':Id,'SalePrice':ANN_prediction})\n",
    "ANN_submission.to_csv('ANN_Submission_file.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
